\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}   
%\usepackage{subfigure}
\usepackage{url}        
\usepackage{amsmath}    
% Some useful/example abbreviations for writing math
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\ypred}{\mathbf{\hat y}}

% Start of added stuff by lukas
%\newtheorem{defi}{Definition}[section]
% Packages
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
% Theorems and definitions
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
% Algorithm description
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
% End of added stuff by lukas

\begin{document}

% Define document title, do NOT write author names
\title{The Title of your Report}
\author{Anonymous Authors}
\maketitle

% Write abstract here
\begin{abstract}
	A short summary of your project. You should change also the title, but do \emph{not} enter any author names or anything that unnecessarily identifies any of the authors. It is suggested you use a similar structure (sections, etc.) as demonstrated in this document, but you can make the section headings more descriptive if you wish. Of course \emph{you should delete all the text in this template and write your own}! -- this text simply provides detailed instructions/hints on how to proceed.

\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}

Describe what you did. Provide access to your anonymized code\footnote{Our code is available here: \url{http://anonymouslinktoyourcode.zip}}.

Note that results should be reproducible using the technologies from the labs (i.e., Python, and selecting among Scikit-Learn, OpenAI Gym, TensorFlow, PyGame, \ldots).

Do not change the formatting (columns, margins, etc). Hint: shared tools like \texttt{http://sharelatex.com/} and \texttt{http://overleaf.com/} are great tools for collaborating on a multi-author report in latex. If you wish to use Word, base it on the IEEE template\footnote{\url{https://www.ieee.org/publications_standards/publications/conferences/2014_04_msw_a4_format.doc}} and convert to \texttt{pdf} for submission. 

\section{Background and Related Work}

Elaborate (in your own words) the background material required to understand your work. It should cover a subset of the topics touched upon in the course. You are encouraged to cite topics in lectures, e.g., structured output prediction in \cite{LectureSOP}, book chapters, e.g., Chapter 9 from \cite{Barber}, or articles from the literature, e.g., \cite{Astar,DeepMindSC2}. Basically, you should prepare the reader to understand what you are about to present in the following sections. Eq.~\eqref{eq:MAP} shows a random equation.
\begin{equation}
	\label{eq:MAP}
	% Note the example \newcommand s defined above which make it faster to write latex math
	\ypred = \argmax_{\y \in \{0,1\}} p(\y|\x)
\end{equation}

\section{The Environment}
In this report we analyze a supply-chain optimization environment that we regard over an infinite amount of periods $t = 0, ...$. The environment consists of one factory (indexed by $0$) and up to $k \in \mathbb{N}$ warehouses (indexed by $j=1,...,k$) where in each period it needs to be decided how many units of a product (e.g. butter) should be produced and how many should be shipped to the individual warehouses. A representation of a network with 5 warehouses is depicted in figure \ref{fig:model_example}. 
\begin{figure}[h]
	\label{fig:model_example}
	\centering
	\includegraphics[width=0.4\columnwidth]{model.png}
	\caption{\label{a_figure}Example of a supply chain network with $k=5$ warehouses ($s_1, .., s_5$) and the factory ($s_0$).}
\end{figure}
The state space consists of $k+1$ elements describing the current stock levels of the factory and the warehouses ($s_j, j=0,...,k$), where each stock level is limited by some capacity $c_j \in \mathbb{N}$. Note that this implies that the factory itself can also build up stock. Furthermore we include an environment process $D \in \{0,..., 4\}$ describing the (stochastic) demand $d_j$ at each warehouse $j = 1,...,k$ with transition probabilities $q_d(d')$ that are unknown by the agent. We assume a discretized version of a sinus-function and some shocks $\epsilon$ where the demand follows a model 
\begin{equation}
	d_{i,t} = \left\lceil 1.5 \sin\left( \frac{2 \pi t}{24} + i\right) + 1.5 + \epsilon \right\rceil, i = 1,..., k
\end{equation}
with $\left\lceil \cdot \right\rceil$ the ceiling function, $P(\epsilon=0) = P(\epsilon=1) = 0.5$ and $t \in \mathbb{N}$. We will include $d \in D$ in our state space since the demand $d_{i, t}$ is correlated with the demand $d_{i, t+1}$. Note that each tupel $(s, d)$ at some point in time $t + 1$ consists of the current state and the last observed demand ($(s_{t+1}, d_t)$). In each period the agent can now set the factories production level for the next period $a_0 \in \{0,.., \rho_{max}\}$ (with a maximum production of $\rho_{max} \in \mathbb{N}$) aswell as the number of products shipped to each location $a_j \in \mathbb{N}^k$ that is naturally limited by the current storage level in the factory ($\sum_{j=1}^{k}a_j \leq s_0$). Based on this information and the demand $d$ we can now describe the state transitions $(s, d, a) \rightarrow T(s,d,a)$ by
\begin{equation}
\label{eq:transition_function}
	\begin{split}
		T(s, d, a) \coloneqq (&\min\{s_0 + a_0 - \sum_{j=1}^{k}a_j, c_0 \}, \\
		&\min\{s_1 + a_1 - d_1, c_1 \}, \\
		&\ ... \\
		&\min\{s_k + a_k - d_k, c_k \}).
\end{split}
\end{equation}
%\begin{equation}
%	\begin{split}
%		s_0' = &\min\{s_0 + a_0 - \sum_{j=1}^{k}a_j \}, \\
%		s_j' = &\min\{s_j + a_j - d_j\}, \ j=1,...,k.
%	\end{split}
%	\end{equation}
The reward within each period consists of the revenue from sold products at a fix price $p$ less production costs $\kappa_{pr}a_0$, storage costs $\sum_{j=0}^{k}\kappa_{st,j} \max\{s_j, 0\}$, penalty costs $\kappa_{pe} \sum_{j=1}^{k}\min\{s_j,0\}$ and transportation costs $\sum_{j=1}^{k} \kappa_{tr, j} \left\lceil a_j / \zeta_j \right\rceil$. We let $p, \kappa_{pr}, \kappa_{pe}, \kappa_{st, j}, \kappa_{tr,j} \in \mathbb{R}_+$ and $\zeta_j \in \mathbb{N}$. We can now define the one-step reward function by
\begin{equation}
\label{eq:one_step_reward}
	\begin{split}
		r(s, d, a) \coloneqq &p \sum_{j=1}^{k}d_i - \kappa_{pr} a_0 - \sum_{j=0}^{k} \kappa_{st, j} \max\{s_j, 0\} \\ 
		&+\kappa_{pe} \sum_{j=1}^{k}\min\{s_j, 0\} - \sum_{j=1}^{k} \kappa_{tr, j} \left\lceil \frac{a_j}{\zeta_j} \right\rceil,
	\end{split}
\end{equation}
with $\left\lceil \cdot \right\rceil$ the ceiling function. Furthermore we chose a discounting factor $\gamma \in (0,1)$ that can be interpreted e.g. as a result of inflation. \newline Based on this model description we can now formulate the task as an infinite horizon markov decision process that is subject to a varying environment. Thereby we receive the tuple $(S, D, A, \mathcal{X}, T, Q, r, \gamma)$ with
\begin{defn} \label{def:modell} \
	\begin{itemize}
		\item[1.] $S \times D \coloneqq \prod\limits_{j=0}^{k} \{s_j \in \mathbb{Z} \ | \ s_j \leq c_j\} \times D, c_j \in \mathbb{N}$ the state space consisting of elements $(s,d) = ((s_0, ... , s_k), (d_1, ..., d_k))$;
		\item[2.] $ A \coloneqq  \{0, ..., \rho_{max} \} \times \mathbb{N}_0^k$ the action space consisting of elements $a = (a_0, ..., a_k)$;
		\item[3.] $ \mathcal{X} (s) \coloneqq \{0, ..., \rho_{max} \} \times \{a \in \mathbb{N}_0^k \ | \ \sum_{j=1}^{k}a_j \leq s_0  \}$, the set of all feasible actions in a state $s$ and $\mathcal{X} \coloneqq \{(s, d , a) \in S \times D \times A \ | \ a \in \mathcal{X} (s)\}$;
		\item[4.] $ T: S \times D \times A \rightarrow S$ the transition function defined by Eq.~\eqref{eq:transition_function};
%			\begin{equation*}
%				\begin{split}
%					T(s, d, a) \coloneqq &(\min\{s_0 + a_0 - \sum_{j=1}^{k}a_j, c_0 \}, \\
%					&\min\{s_1 + a_1 - d_1, c_1 \}, \\
%					&\ ... \\
%					&\min\{s_k + a_k - d_k, c_k \});
%				\end{split}
%			\end{equation*}
		\item[5.] $ Q: \mathcal{X} \times S \times D \rightarrow [0,1]  $ the transition probabilities with $Q(s', d'\ |\ s, d, a) \coloneqq q_d(d')$ for $s' = T(s, d, a)$ and $0$ otherwise;
		\item[6.] $ r: S \times D \times A \rightarrow \mathbb{R} $ the one-step reward function as described in Eq.~\eqref{eq:one_step_reward};
%			\begin{equation*}
%				\begin{split}
%					r(s, d, a) \coloneqq &p \sum_{j=1}^{k}d_i - \kappa_{pr} a_0 - \sum_{j=0}^{k} \kappa_{st, j} \max\{s_j, 0\} \\ 
%					&-\kappa_{pe} \sum_{j=1}^{k}\min\{s_j, 0\} - \sum_{j=1}^{k} \kappa_{tr, j} \left\lceil \frac{a_j}{\zeta_j} \right\rceil;
%				\end{split}
%			\end{equation*}
		\item[7.] $\gamma \in (0, 1) $ the discounting factor.
	\end{itemize}
\end{defn}
For the calculation of the expected discounted revenue for each tuple $(s, d)$ we obtain the value function $V$ with\\
\begin{equation}
	\label{eq:ValueFunction}
	\begin{split}
		V(s,d) = &\max_{a \in \mathcal{X} (s)} \{ r(s, d, a)  \\
		&+ \gamma \sum_{d' \in D} q_d(d') V(T(s, d, a), d') \}, \\ 
		&s \in S, d \in D. 
	\end{split}
\end{equation}
\section{The Agent}

%The agent you designed for your environment. Justify your choice and design and explain briefly how you implemented/configured it. Naturally, if you took a %ready-made environment, you should invert relatively much more effort into this section than the previous one.
In this paper we evaluated the performance of three agents based on different algorithms: a heuristic based on the $(s^{-}, Q)$-Policy (add ref.) that we use as the baseline for performance evaluation, an approximate SARSA (add ref.) algorithm  and an implementation of the REINFORCE (add ref.) algorithm. \newline
\textbf{The $(s^{-}, Q)$-Policy} based agent is not smart in the way that it does not learn over time. Due to the popularity of the $(s^{-}, Q)$-Policy in practice we use it as a baseline for the performance evaluation of the other agents. In the $(s^{-}, Q)$ policy we iterate over $s_1, ..., s_k$ and replenish the respective warehouses by some amount $Q_i$ if the current stock is below a level $s_i^{-}$ and there is still stock left in the factory $s_0$. At the and we set the production level for the next period to $Q_0$ if the new $s_0 < s_0^{-}$ and $0$ otherwise. The thresholds $s^{-}$ and replenishment levels $Q$ need to be set by the user when initializing the agent.
\newline
\textbf{Approximate SARSA} uses a linear approximation $Q_{\theta}(s, a) = \theta^T \phi(s,a)$ of the actual Q-function $Q^{\pi}$. We chose this method as it solves the problem of exponentially growing state- and action- spaces that is likely to occur in multidimensional environments. Furthermore it allows us to use our knowledge of the environment (especially the structure of the reward function $r(s, d, a)$) to design $\phi(s, a)$ such that it preserves some of the MDPs structure. The algorithm is implemented as depicted in (add ref.). %Algorithm \ref{algo:approximate_sarsa}.
%\begin{algorithm}
%	\label{algo:approximate_sarsa}
%	\caption{Approximate SARSA}\label{euclid}
%	\begin{algorithmic}[1]
%		\Procedure{myprod}{}
%		\State $\textit{stringlen} \gets \text{length of }\textit{string}$
%		\State $i \gets \textit{patlen}$
%		\BState \emph{top}:
%		\If {$i > \textit{stringlen}$} \Return false
%		\EndIf
%		\State $j \gets \textit{patlen}$
%		\BState \emph{loop}:
%		\If {$\textit{string}(i) = \textit{path}(j)$}
%		\State $j \gets j-1$.
%		\State $i \gets i-1$.
%		\State \textbf{goto} \emph{loop}.
%		\State \textbf{close};
%		\EndIf
%		\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%		\State \textbf{goto} \emph{top}.
%		\EndProcedure
%	\end{algorithmic}
%\end{algorithm}
The underlying intuiton for the definition of $\phi$ is to value states $s$ based on an approximation of the individual storage levels and the sales price $p$. We assume that the stock in each warehouse $s_1,..., s_k$ will be roughly equivalent to the current stock level minus the current demand (which is used as a proxy of the demand in the next step). For the factory $s_0$ we solely regard the current storage level but deduct the production cost $\kappa_{pr}$ from the sales price. Based on this intuition we can define $\phi: S \times D \times A \rightarrow \mathbb{R}^{k+1}$ by
\begin{equation}
	\label{eq:sarsa_phi}
	\begin{split}
	\phi(s, a) \coloneqq (&(s_0 + a_0)(p - \kappa_{pr}), \\
	&(s_1 + a_1 - d_1)p, \\
	&\ ... \\
	&p (s_k + a_k - d_k)p ).
	\end{split}
\end{equation} 
When testing the approximate SARSA algorihm we found that originally the parameters $\theta$ would increase until computations became numerically unstable. To avoid this issue, after each update of $\theta$ we apply the normalization $\theta_i = \theta_i (\sum_{i=1}^{k+1} \theta_i)^{-1}$. Since this only affects the absolute value of $Q_{\theta}(s,a)$ but not its order the normalization does not change the behavior of the agent.
\section{Results and Discussion}



This is one of the most important sections. You put your agent to the test in the environment, you show -- and most importantly -- you interpret the results.

\subsection{Performance of your Agent in your Environment}

Show plots, graphs, tables (e.g., Table~\ref{a_table}), etc. You may wish to encourage readers to reproduce results for themselves, e.g., run \texttt{runDemo.py} in our source code. Show how your agent performs well, or, if it doesn't perform well, it is better to explain why (this is a result in itself!). In any case, you \emph{must} highlight the weaknesses of your agent as well as its strengths. \newline


\begin{table}[h]
	\caption{\label{a_table}This table is just an example.}
	\centering
	\begin{tabular}{lll}
		\hline
		\textbf{Environment config.} & \textbf{Standard SARSA} & \textbf{Our Improved Agent}  \\
		\hline
		Simulation 1        & 10             & 15 \\
		Simulation 2        & 12             & 11 \\
		\hline
	\end{tabular}
\end{table}

\subsection{Performance of your Agent in the ALife Environment}

You deploy your agent in the ALife\footnote{\url{https://github.com/jmread/alife}} environment (a random screenshot shown in Figure~\ref{a_figure}). Does it work well? Why? Why not? Justify the adaptation you think is best.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\columnwidth]{alife.png}
	\caption{\label{a_figure}An example figure}
\end{figure}


\section{Conclusion and Future Work}
	This section summarizes the paper: Your environment and agent, its strength and its weaknesses. Also remark about what would be the next steps you would take if you or someone else were to continue/extend this project. 
	Note that for the initial submission you are limited strictly to 4 pages (double column), \emph{not including references}. An extra page will be allowed for final submission (after the initial reviews). 

% The bibliography:
\begin{thebibliography}{4}

	\bibitem{Barber} % Book
	D.~Barber. Bayesian Reasoning and Machine Learning,
	{\em Cambridge University Press}, 2012.

	\bibitem{LectureSOP} % Web document
		J.~Read. Lecture III - Structured Output Prediction and Search. \textit{INF581 Advanced Topics in Artificial Intelligence}, 2018.

	\bibitem{Astar}
	D.~Mena et al. A family of admissible heuristics for A* to perform inference in probabilistic classifier chains.
	{\em Machine Learning}, vol. 106, no. 1, pp 143-169, 2017.

	\bibitem{DeepMindSC2}
	O.~Vinyals et al. StarCraft {II:} {A} New Challenge for Reinforcement Learning.
	\url{https://arxiv.org/abs/1708.04782}, 2017. 

\end{thebibliography}

% Your document ends here!
\end{document}
