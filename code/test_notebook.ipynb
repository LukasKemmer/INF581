{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run evaluate_agent2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actions:  81\n",
      "Episode  0\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [8 1 2 1]\n",
      "Reward:         -17.49\n",
      "Next state:     [9 1 2 1 0 0 0]\n",
      "Episode reward: -17.49\n",
      "========= Step:   1 =========\n",
      "State:          [9 1 2 1 0 0 0]\n",
      "Action:         [8 1 1 0]\n",
      "Reward:         32.85\n",
      "Next state:     [15  0 -1 -3  2  4  4]\n",
      "Episode reward: 12.07\n",
      "========= Step:   2 =========\n",
      "State:          [15  0 -1 -3  2  4  4]\n",
      "Action:         [0 1 1 2]\n",
      "Reward:         26.89\n",
      "Next state:     [11 -2 -3 -4  3  3  3]\n",
      "Episode reward: 33.86\n",
      "========= Step:   3 =========\n",
      "State:          [11 -2 -3 -4  3  3  3]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         26.81\n",
      "Next state:     [19 -5 -7 -8  3  4  4]\n",
      "Episode reward: 53.4\n",
      "========= Step:   4 =========\n",
      "State:          [19 -5 -7 -8  3  4  4]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         11.86\n",
      "Next state:     [14 -6 -9 -9  3  3  3]\n",
      "Episode reward: 61.18\n",
      "========= Step:   5 =========\n",
      "State:          [14 -6 -9 -9  3  3  3]\n",
      "Action:         [4 0 1 1]\n",
      "Reward:         2.84\n",
      "Next state:     [ 16  -9 -11 -11   3   3   3]\n",
      "Episode reward: 62.86\n",
      "========= Step:   6 =========\n",
      "State:          [ 16  -9 -11 -11   3   3   3]\n",
      "Action:         [0 1 2 2]\n",
      "Reward:         -3.11\n",
      "Next state:     [ 11 -11 -13 -10   3   4   1]\n",
      "Episode reward: 61.21\n",
      "========= Step:   7 =========\n",
      "State:          [ 11 -11 -13 -10   3   4   1]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -18.19\n",
      "Next state:     [ 19 -14 -15 -11   3   2   1]\n",
      "Episode reward: 52.51\n",
      "========= Step:   8 =========\n",
      "State:          [ 19 -14 -15 -11   3   2   1]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -15.16\n",
      "Next state:     [ 16 -16 -17 -11   4   2   1]\n",
      "Episode reward: 45.98\n",
      "========= Step:   9 =========\n",
      "State:          [ 16 -16 -17 -11   4   2   1]\n",
      "Action:         [0 2 0 2]\n",
      "Reward:         -14.12\n",
      "Next state:     [ 12 -18 -19 -11   4   2   2]\n",
      "Episode reward: 40.51\n",
      "========= Step:  10 =========\n",
      "State:          [ 12 -18 -19 -11   4   2   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -28.2\n",
      "Next state:     [ 20 -21 -21 -13   3   2   2]\n",
      "Episode reward: 30.68\n",
      "========= Step:  11 =========\n",
      "State:          [ 20 -21 -21 -13   3   2   2]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -34.17\n",
      "Next state:     [ 17 -21 -23 -14   2   2   2]\n",
      "Episode reward: 19.95\n",
      "========= Step:  12 =========\n",
      "State:          [ 17 -21 -23 -14   2   2   2]\n",
      "Action:         [0 1 1 1]\n",
      "Reward:         -36.14\n",
      "Next state:     [ 14 -23 -24 -15   3   2   2]\n",
      "Episode reward: 9.75\n",
      "========= Step:  13 =========\n",
      "State:          [ 14 -23 -24 -15   3   2   2]\n",
      "Action:         [0 1 2 1]\n",
      "Reward:         -51.1\n",
      "Next state:     [ 10 -24 -23 -15   2   1   1]\n",
      "Episode reward: -3.24\n",
      "========= Step:  14 =========\n",
      "State:          [ 10 -24 -23 -15   2   1   1]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -55.16\n",
      "Next state:     [ 16 -25 -24 -16   1   2   2]\n",
      "Episode reward: -15.86\n",
      "========= Step:  15 =========\n",
      "State:          [ 16 -25 -24 -16   1   2   2]\n",
      "Action:         [8 1 1 2]\n",
      "Reward:         -66.2\n",
      "Next state:     [ 20 -25 -24 -15   1   1   1]\n",
      "Episode reward: -29.49\n",
      "========= Step:  16 =========\n",
      "State:          [ 20 -25 -24 -15   1   1   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -49.19\n",
      "Next state:     [ 19 -25 -25 -17   1   1   2]\n",
      "Episode reward: -38.61\n",
      "========= Step:  17 =========\n",
      "State:          [ 19 -25 -25 -17   1   1   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -50.18\n",
      "Next state:     [ 18 -26 -27 -18   1   2   2]\n",
      "Episode reward: -46.97\n",
      "========= Step:  18 =========\n",
      "State:          [ 18 -26 -27 -18   1   2   2]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -50.15\n",
      "Next state:     [ 15 -25 -29 -20   1   2   3]\n",
      "Episode reward: -54.5\n",
      "========= Step:  19 =========\n",
      "State:          [ 15 -25 -29 -20   1   2   3]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -45.12\n",
      "Next state:     [ 12 -24 -32 -23   1   3   4]\n",
      "Episode reward: -60.6\n",
      "========= Step:  20 =========\n",
      "State:          [ 12 -24 -32 -23   1   3   4]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -55.2\n",
      "Next state:     [ 20 -25 -35 -27   1   3   4]\n",
      "Episode reward: -67.31\n",
      "Update: [[ 70.     5.     0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 66.     5.     0.     0.     0.     0.     0.     0.   -17.49]\n",
      " [ 14.     9.     1.     2.     1.     0.     0.     0.    32.85]\n",
      " [ 54.    15.     0.    -1.    -3.     2.     4.     4.    26.89]\n",
      " [ 23.    11.    -2.    -3.    -4.     3.     3.     3.    26.81]\n",
      " [ 31.    19.    -5.    -7.    -8.     3.     4.     4.    11.86]\n",
      " [ 17.    14.    -6.    -9.    -9.     3.     3.     3.     2.84]\n",
      " [ 54.    16.    -9.   -11.   -11.     3.     3.     3.    -3.11]\n",
      " [ 19.    11.   -11.   -13.   -10.     3.     4.     1.   -18.19]\n",
      " [ 20.    19.   -14.   -15.   -11.     3.     2.     1.   -15.16]\n",
      " [ 54.    16.   -16.   -17.   -11.     4.     2.     1.   -14.12]\n",
      " [ 19.    12.   -18.   -19.   -11.     4.     2.     2.   -28.2 ]\n",
      " [ 13.    20.   -21.   -21.   -13.     3.     2.     2.   -34.17]\n",
      " [ 16.    17.   -21.   -23.   -14.     2.     2.     2.   -36.14]\n",
      " [ 58.    14.   -23.   -24.   -15.     3.     2.     2.   -51.1 ]\n",
      " [ 68.    10.   -24.   -23.   -15.     2.     1.     1.   -55.16]\n",
      " [  9.    16.   -25.   -24.   -16.     1.     2.     2.   -66.2 ]\n",
      " [  1.    20.   -25.   -24.   -15.     1.     1.     1.   -49.19]\n",
      " [ 19.    19.   -25.   -25.   -17.     1.     1.     2.   -50.18]\n",
      " [ 19.    18.   -26.   -27.   -18.     1.     2.     2.   -50.15]\n",
      " [ 54.    15.   -25.   -29.   -20.     1.     2.     3.   -45.12]\n",
      " [ 19.    12.   -24.   -32.   -23.     1.     3.     4.   -55.2 ]\n",
      " [ 19.    20.   -25.   -35.   -27.     1.     3.     4.   -58.17]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.  ]]\n",
      "========= Step:  21 =========\n",
      "State:          [ 20 -25 -35 -27   1   3   4]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -58.17\n",
      "Next state:     [ 17 -24 -38 -30   1   3   4]\n",
      "Episode reward: -73.67\n",
      "========= Step:  22 =========\n",
      "State:          [ 17 -24 -38 -30   1   3   4]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -59.14\n",
      "Next state:     [ 14 -24 -41 -33   2   3   4]\n",
      "Episode reward: -79.5\n",
      "========= Step:  23 =========\n",
      "State:          [ 14 -24 -41 -33   2   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -58.14\n",
      "Next state:     [ 14 -27 -45 -36   3   4   3]\n",
      "Episode reward: -84.65\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [8 0 2 0]\n",
      "Reward:         -11.31\n",
      "Next state:     [11  0  2  0  0  0  0]\n",
      "Episode reward: -11.31\n",
      "========= Step:   1 =========\n",
      "State:          [11  0  2  0  0  0  0]\n",
      "Action:         [0 1 2 2]\n",
      "Reward:         28.84\n",
      "Next state:     [ 6 -1  1 -1  2  3  3]\n",
      "Episode reward: 14.65\n",
      "========= Step:   2 =========\n",
      "State:          [ 6 -1  1 -1  2  3  3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         37.92\n",
      "Next state:     [ 8 -5 -1 -4  4  4  3]\n",
      "Episode reward: 45.36\n",
      "========= Step:   3 =========\n",
      "State:          [ 8 -5 -1 -4  4  4  3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         35.93\n",
      "Next state:     [ 7 -9 -4 -8  4  4  4]\n",
      "Episode reward: 71.55\n",
      "========= Step:   4 =========\n",
      "State:          [ 7 -9 -4 -8  4  4  4]\n",
      "Action:         [8 2 2 1]\n",
      "Reward:         10.9\n",
      "Next state:     [ 10 -11  -6 -10   4   4   3]\n",
      "Episode reward: 78.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\evaluate_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Select a new action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0maction_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Update episode reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# save the allowed actions for that state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mallowed_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_allowed_actions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallowed_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py\u001b[0m in \u001b[0;36mallowed_actions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m#if self.discrete2continuous[i]  in  action_space:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#a_allowed[i]= 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscrete2continuous\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[0ma_allowed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\supply_distribution.py\u001b[0m in \u001b[0;36maction_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space_recur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\supply_distribution.py\u001b[0m in \u001b[0;36maction_space_recur\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mprod_being_send\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0ms_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprod_being_send\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mproduction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_prod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcap_store\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms_0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[0mfeasible_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeasible_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run evaluate_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[10,20],[100,200]])\n",
    "[1,2] in a.tolist()\n",
    "[1,20] in a.tolist()\n",
    "[1,20] in a.tolist()\n",
    "[1,42] in a.tolist()\n",
    "[1,2,3] in a.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array([4,3,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
