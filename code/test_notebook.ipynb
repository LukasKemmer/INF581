{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run evaluate_agent2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0 0 0 0]\n",
      "Action:         [0 1 0 1]\n",
      "Reward:         -5.3\n",
      "Next state:     [3 1 0 1 0 0 0 0 0 0]\n",
      "Episode reward: -5.3\n",
      "========= Step:   1 =========\n",
      "State:          [3 1 0 1 0 0 0 0 0 0]\n",
      "Action:         [8 0 1 0]\n",
      "Reward:         15.0\n",
      "Next state:     [10 -1 -3 -3  2  4  4  0  0  0]\n",
      "Episode reward: 8.2\n",
      "========= Step:   2 =========\n",
      "State:          [10 -1 -3 -3  2  4  4  0  0  0]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         8.2\n",
      "Next state:     [ 8 -2 -6 -7  3  3  4  2  4  4]\n",
      "Episode reward: 14.84\n",
      "========= Step:   3 =========\n",
      "State:          [ 8 -2 -6 -7  3  3  4  2  4  4]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -9.6\n",
      "Next state:     [  6  -3  -9 -10   3   3   3   3   3   4]\n",
      "Episode reward: 7.84\n",
      "========= Step:   4 =========\n",
      "State:          [  6  -3  -9 -10   3   3   3   3   3   4]\n",
      "Action:         [8 2 0 1]\n",
      "Reward:         -33.1\n",
      "Next state:     [ 11  -4 -12 -12   3   3   3   3   3   3]\n",
      "Episode reward: -13.87\n",
      "========= Step:   5 =========\n",
      "State:          [ 11  -4 -12 -12   3   3   3   3   3   3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -37.3\n",
      "Next state:     [ 13  -8 -14 -15   4   4   3   3   3   3]\n",
      "Episode reward: -35.9\n",
      "========= Step:   6 =========\n",
      "State:          [ 13  -8 -14 -15   4   4   3   3   3   3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -50.1\n",
      "Next state:     [ 11  -9 -18 -19   3   4   4   4   4   3]\n",
      "Episode reward: -62.52\n",
      "========= Step:   7 =========\n",
      "State:          [ 11  -9 -18 -19   3   4   4   4   4   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -71.0\n",
      "Next state:     [ 10 -13 -20 -23   4   3   4   3   4   4]\n",
      "Episode reward: -96.48\n",
      "========= Step:   8 =========\n",
      "State:          [ 10 -13 -20 -23   4   3   4   3   4   4]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -95.2\n",
      "Next state:     [ 12 -16 -21 -27   3   3   4   4   3   4]\n",
      "Episode reward: -137.46\n",
      "========= Step:   9 =========\n",
      "State:          [ 12 -16 -21 -27   3   3   4   4   3   4]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -113.4\n",
      "Next state:     [ 14 -19 -22 -30   3   3   3   3   3   4]\n",
      "Episode reward: -181.4\n",
      "========= Step:  10 =========\n",
      "State:          [ 14 -19 -22 -30   3   3   3   3   3   4]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -122.2\n",
      "Next state:     [ 12 -21 -25 -32   4   3   2   3   3   3]\n",
      "Episode reward: -224.01\n",
      "========= Step:  11 =========\n",
      "State:          [ 12 -21 -25 -32   4   3   2   3   3   3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -141.4\n",
      "Next state:     [ 14 -25 -25 -35   4   2   3   4   3   2]\n",
      "Episode reward: -268.38\n",
      "========= Step:  12 =========\n",
      "State:          [ 14 -25 -25 -35   4   2   3   4   3   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -157.6\n",
      "Next state:     [ 16 -28 -26 -37   3   3   2   4   2   3]\n",
      "Episode reward: -312.89\n",
      "========= Step:  13 =========\n",
      "State:          [ 16 -28 -26 -37   3   3   2   4   2   3]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -171.7\n",
      "Next state:     [ 17 -29 -29 -37   3   3   1   3   3   2]\n",
      "Episode reward: -356.53\n",
      "========= Step:  14 =========\n",
      "State:          [ 17 -29 -29 -37   3   3   1   3   3   2]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -181.8\n",
      "Next state:     [ 18 -30 -30 -38   3   1   2   3   3   1]\n",
      "Episode reward: -398.12\n",
      "========= Step:  15 =========\n",
      "State:          [ 18 -30 -30 -38   3   1   2   3   3   1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -186.6\n",
      "Next state:     [ 16 -29 -32 -39   1   2   1   3   1   2]\n",
      "Episode reward: -436.54\n",
      "========= Step:  16 =========\n",
      "State:          [ 16 -29 -32 -39   1   2   1   3   1   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -191.8\n",
      "Next state:     [ 18 -31 -32 -41   2   2   2   1   2   1]\n",
      "Episode reward: -472.08\n",
      "========= Step:  17 =========\n",
      "State:          [ 18 -31 -32 -41   2   2   2   1   2   1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -198.6\n",
      "Next state:     [ 16 -30 -33 -43   1   1   2   2   2   2]\n",
      "Episode reward: -505.2\n",
      "========= Step:  18 =========\n",
      "State:          [ 16 -30 -33 -43   1   1   2   2   2   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -209.8\n",
      "Next state:     [ 18 -31 -32 -44   1   1   1   1   1   2]\n",
      "Episode reward: -536.69\n",
      "========= Step:  19 =========\n",
      "State:          [ 18 -31 -32 -44   1   1   1   1   1   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -210.0\n",
      "Next state:     [ 20 -32 -31 -46   1   1   2   1   1   1]\n",
      "Episode reward: -565.06\n",
      "========= Step:  20 =========\n",
      "State:          [ 20 -32 -31 -46   1   1   2   1   1   1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -206.8\n",
      "Next state:     [ 18 -32 -32 -48   2   1   2   1   1   2]\n",
      "Episode reward: -590.2\n",
      "========= Step:  21 =========\n",
      "State:          [ 18 -32 -32 -48   2   1   2   1   1   2]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -217.9\n",
      "Next state:     [ 19 -31 -34 -49   1   2   2   2   1   2]\n",
      "Episode reward: -614.05\n",
      "========= Step:  22 =========\n",
      "State:          [ 19 -31 -34 -49   1   2   2   2   1   2]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -216.7\n",
      "Next state:     [ 17 -31 -36 -50   2   2   1   1   2   2]\n",
      "Episode reward: -635.39\n",
      "========= Step:  23 =========\n",
      "State:          [ 17 -31 -36 -50   2   2   1   1   2   2]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -227.8\n",
      "Next state:     [ 18 -30 -38 -51   1   2   2   2   2   1]\n",
      "Episode reward: -655.58\n",
      "========= Step:  24 =========\n",
      "State:          [ 18 -30 -38 -51   1   2   2   2   2   1]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -228.0\n",
      "Next state:     [ 20 -32 -39 -53   2   3   2   1   2   2]\n",
      "Episode reward: -673.76\n",
      "========= Step:  25 =========\n",
      "State:          [ 20 -32 -39 -53   2   3   2   1   2   2]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -232.8\n",
      "Next state:     [ 18 -33 -41 -55   3   2   2   2   3   2]\n",
      "Episode reward: -690.48\n",
      "========= Step:  26 =========\n",
      "State:          [ 18 -33 -41 -55   3   2   2   2   3   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -244.0\n",
      "Next state:     [ 20 -36 -41 -59   3   2   4   3   2   2]\n",
      "Episode reward: -706.24\n",
      "========= Step:  27 =========\n",
      "State:          [ 20 -36 -41 -59   3   2   4   3   2   2]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -250.8\n",
      "Next state:     [ 18 -37 -44 -63   3   3   4   3   2   4]\n",
      "Episode reward: -720.82\n",
      "========= Step:  28 =========\n",
      "State:          [ 18 -37 -44 -63   3   3   4   3   2   4]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -272.0\n",
      "Next state:     [ 20 -41 -45 -66   4   3   3   3   3   4]\n",
      "Episode reward: -735.06\n",
      "========= Step:  29 =========\n",
      "State:          [ 20 -41 -45 -66   4   3   3   3   3   4]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -282.8\n",
      "Next state:     [ 18 -42 -48 -70   3   3   4   4   3   3]\n",
      "Episode reward: -748.38\n",
      "========= Step:  30 =========\n",
      "State:          [ 18 -42 -48 -70   3   3   4   4   3   3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -300.0\n",
      "Next state:     [ 20 -46 -50 -74   4   4   4   3   3   4]\n",
      "Episode reward: -761.1\n",
      "========= Step:  31 =========\n",
      "State:          [ 20 -46 -50 -74   4   4   4   3   3   4]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -316.8\n",
      "Next state:     [ 18 -48 -53 -78   4   3   4   4   4   4]\n",
      "Episode reward: -773.18\n",
      "========= Step:  32 =========\n",
      "State:          [ 18 -48 -53 -78   4   3   4   4   4   4]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -343.9\n",
      "Next state:     [ 19 -49 -56 -80   3   3   3   4   3   4]\n",
      "Episode reward: -784.99\n",
      "========= Step:  33 =========\n",
      "State:          [ 19 -49 -56 -80   3   3   3   4   3   4]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -352.0\n",
      "Next state:     [ 20 -51 -60 -82   4   4   3   3   3   3]\n",
      "Episode reward: -795.87\n",
      "========= Step:  34 =========\n",
      "State:          [ 20 -51 -60 -82   4   4   3   3   3   3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -362.8\n",
      "Next state:     [ 18 -52 -64 -86   3   4   4   4   4   3]\n",
      "Episode reward: -805.96\n",
      "========= Step:  35 =========\n",
      "State:          [ 18 -52 -64 -86   3   4   4   4   4   3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -390.0\n",
      "Next state:     [ 20 -55 -65 -89   3   3   3   3   4   4]\n",
      "Episode reward: -815.72\n",
      "========= Step:  36 =========\n",
      "State:          [ 20 -55 -65 -89   3   3   3   3   4   4]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -394.8\n",
      "Next state:     [ 18 -57 -69 -92   4   4   3   3   3   3]\n",
      "Episode reward: -824.62\n",
      "========= Step:  37 =========\n",
      "State:          [ 18 -57 -69 -92   4   4   3   3   3   3]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -424.0\n",
      "Next state:     [ 20 -60 -70 -94   3   3   2   4   4   3]\n",
      "Episode reward: -833.21\n",
      "========= Step:  38 =========\n",
      "State:          [ 20 -60 -70 -94   3   3   2   4   4   3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -430.8\n",
      "Next state:     [ 18 -61 -72 -97   3   2   3   3   3   2]\n",
      "Episode reward: -841.07\n",
      "========= Step:  39 =========\n",
      "State:          [ 18 -61 -72 -97   3   2   3   3   3   2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -448.0\n",
      "Next state:     [ 20 -64 -73 -99   3   3   2   3   2   3]\n",
      "Episode reward: -848.43\n",
      "========= Step:  40 =========\n",
      "State:          [ 20 -64 -73 -99   3   3   2   3   2   3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -458.8\n",
      "Next state:     [  18  -65  -75 -100    3    2    1    3    3    2]\n",
      "Episode reward: -855.21\n",
      "========= Step:  41 =========\n",
      "State:          [  18  -65  -75 -100    3    2    1    3    3    2]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -476.0\n",
      "Next state:     [  20  -67  -74 -101    2    1    1    3    2    1]\n",
      "Episode reward: -861.55\n",
      "========= Step:  42 =========\n",
      "State:          [  20  -67  -74 -101    2    1    1    3    2    1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -474.8\n",
      "Next state:     [  18  -67  -75 -102    2    1    1    2    1    1]\n",
      "Episode reward: -867.23\n",
      "========= Step:  43 =========\n",
      "State:          [  18  -67  -75 -102    2    1    1    2    1    1]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -484.0\n",
      "Next state:     [  20  -68  -75 -103    1    2    1    2    1    1]\n",
      "Episode reward: -872.45\n",
      "========= Step:  44 =========\n",
      "State:          [  20  -68  -75 -103    1    2    1    2    1    1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -480.8\n",
      "Next state:     [  18  -68  -77 -104    2    2    1    1    2    1]\n",
      "Episode reward: -877.11\n",
      "========= Step:  45 =========\n",
      "State:          [  18  -68  -77 -104    2    2    1    1    2    1]\n",
      "Action:         [4 0 2 0]\n",
      "Reward:         -492.0\n",
      "Next state:     [  20  -70  -76 -106    2    1    2    2    2    1]\n",
      "Episode reward: -881.4\n",
      "========= Step:  46 =========\n",
      "State:          [  20  -70  -76 -106    2    1    2    2    2    1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -490.8\n",
      "Next state:     [  18  -70  -78 -108    2    2    2    2    1    2]\n",
      "Episode reward: -885.26\n",
      "========= Step:  47 =========\n",
      "State:          [  18  -70  -78 -108    2    2    2    2    1    2]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -505.9\n",
      "Next state:     [  19  -70  -80 -108    2    2    1    2    2    2]\n",
      "Episode reward: -888.83\n",
      "========= Step:  48 =========\n",
      "State:          [  19  -70  -80 -108    2    2    1    2    2    2]\n",
      "Action:         [4 2 0 1]\n",
      "Reward:         -512.0\n",
      "Next state:     [  20  -70  -81 -108    2    1    1    2    2    1]\n",
      "Episode reward: -892.09\n",
      "========= Step:  49 =========\n",
      "State:          [  20  -70  -81 -108    2    1    1    2    2    1]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -506.8\n",
      "Next state:     [  18  -69  -82 -111    1    1    3    2    1    1]\n",
      "Episode reward: -894.99\n",
      "========= Step:  50 =========\n",
      "State:          [  18  -69  -82 -111    1    1    3    2    1    1]\n",
      "Action:         [4 0 2 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:         -516.0\n",
      "Next state:     [  20  -71  -82 -113    2    2    2    1    1    3]\n",
      "Episode reward: -897.65\n",
      "Algorithm time:  2.974022388458252  seconds!\n",
      "timeforallowedact =  2.8389222621917725\n",
      "time for update =  0.051169395446777344\n",
      "========= Step:  51 =========\n",
      "State:          [  20  -71  -82 -113    2    2    2    1    1    3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         -514.8\n",
      "Next state:     [  18  -71  -85 -116    2    3    3    2    2    2]\n",
      "Episode reward: -900.04\n",
      "Episode  1\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0 0 0 0]\n",
      "Action:         [8 1 0 0]\n",
      "Reward:         -10.7\n",
      "Next state:     [12  1  0  0  0  0  0  0  0  0]\n",
      "Episode reward: -10.7\n",
      "========= Step:   1 =========\n",
      "State:          [12  1  0  0  0  0  0  0  0  0]\n",
      "Action:         [8 1 1 1]\n",
      "Reward:         10.3\n",
      "Next state:     [17  0 -2 -3  2  3  4  0  0  0]\n",
      "Episode reward: -1.43\n",
      "========= Step:   2 =========\n",
      "State:          [17  0 -2 -3  2  3  4  0  0  0]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         9.5\n",
      "Next state:     [15 -4 -4 -6  4  3  4  2  3  4]\n",
      "Episode reward: 6.27\n",
      "========= Step:   3 =========\n",
      "State:          [15 -4 -4 -6  4  3  4  2  3  4]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -9.2\n",
      "Next state:     [12 -6 -7 -7  3  3  3  4  3  4]\n",
      "Episode reward: -0.44\n",
      "========= Step:   4 =========\n",
      "State:          [12 -6 -7 -7  3  3  3  4  3  4]\n",
      "Action:         [8 0 1 2]\n",
      "Reward:         -30.7\n",
      "Next state:     [17 -9 -9 -8  3  3  3  3  3  3]\n",
      "Episode reward: -20.58\n",
      "========= Step:   5 =========\n",
      "State:          [17 -9 -9 -8  3  3  3  3  3  3]\n",
      "Action:         [0 2 1 0]\n",
      "Reward:         -26.4\n",
      "Next state:     [ 14 -11 -12 -12   4   4   4   3   3   3]\n",
      "Episode reward: -36.17\n",
      "========= Step:   6 =========\n",
      "State:          [ 14 -11 -12 -12   4   4   4   3   3   3]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -52.5\n",
      "Next state:     [ 15 -15 -14 -14   4   3   4   4   4   4]\n",
      "Episode reward: -64.07\n",
      "========= Step:   7 =========\n",
      "State:          [ 15 -15 -14 -14   4   3   4   4   4   4]\n",
      "Action:         [0 2 1 0]\n",
      "Reward:         -62.2\n",
      "Next state:     [ 12 -17 -17 -17   4   4   3   4   3   4]\n",
      "Episode reward: -93.82\n",
      "========= Step:   8 =========\n",
      "State:          [ 12 -17 -17 -17   4   4   3   4   3   4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -87.6\n",
      "Next state:     [ 16 -20 -20 -21   3   3   4   4   4   3]\n",
      "Episode reward: -131.53\n",
      "========= Step:   9 =========\n",
      "State:          [ 16 -20 -20 -21   3   3   4   4   4   3]\n",
      "Action:         [4 1 0 0]\n",
      "Reward:         -106.9\n",
      "Next state:     [ 19 -23 -23 -24   4   3   3   3   3   4]\n",
      "Episode reward: -172.95\n",
      "========= Step:  10 =========\n",
      "State:          [ 19 -23 -23 -24   4   3   3   3   3   4]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -127.0\n",
      "Next state:     [ 20 -26 -25 -25   3   3   3   4   3   3]\n",
      "Episode reward: -217.23\n",
      "========= Step:  11 =========\n",
      "State:          [ 20 -26 -25 -25   3   3   3   4   3   3]\n",
      "Action:         [0 2 2 1]\n",
      "Reward:         -133.5\n",
      "Next state:     [ 15 -27 -26 -26   3   3   2   3   3   3]\n",
      "Episode reward: -259.12\n",
      "========= Step:  12 =========\n",
      "State:          [ 15 -27 -26 -26   3   3   2   3   3   3]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -147.7\n",
      "Next state:     [ 17 -29 -28 -28   3   2   3   3   3   2]\n",
      "Episode reward: -300.84\n",
      "========= Step:  13 =========\n",
      "State:          [ 17 -29 -28 -28   3   2   3   3   3   2]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -163.9\n",
      "Next state:     [ 19 -30 -30 -29   2   2   2   3   2   3]\n",
      "Episode reward: -342.5\n",
      "========= Step:  14 =========\n",
      "State:          [ 19 -30 -30 -29   2   2   2   3   2   3]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -165.4\n",
      "Next state:     [ 14 -30 -30 -29   2   1   2   2   2   2]\n",
      "Episode reward: -380.34\n",
      "========= Step:  15 =========\n",
      "State:          [ 14 -30 -30 -29   2   1   2   2   2   2]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -173.6\n",
      "Next state:     [ 16 -31 -31 -30   2   1   2   2   1   2]\n",
      "Episode reward: -416.08\n",
      "========= Step:  16 =========\n",
      "State:          [ 16 -31 -31 -30   2   1   2   2   1   2]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -181.8\n",
      "Next state:     [ 18 -31 -32 -31   1   1   2   2   1   2]\n",
      "Episode reward: -449.77\n",
      "========= Step:  17 =========\n",
      "State:          [ 18 -31 -32 -31   1   1   2   2   1   2]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -184.0\n",
      "Next state:     [ 20 -32 -34 -31   2   2   1   1   1   2]\n",
      "Episode reward: -480.45\n",
      "========= Step:  18 =========\n",
      "State:          [ 20 -32 -34 -31   2   2   1   1   1   2]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -181.5\n",
      "Next state:     [ 15 -32 -34 -31   2   1   2   2   2   1]\n",
      "Episode reward: -507.7\n",
      "========= Step:  19 =========\n",
      "State:          [ 15 -32 -34 -31   2   1   2   2   2   1]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -191.9\n",
      "Next state:     [ 19 -34 -35 -32   2   1   1   2   1   2]\n",
      "Episode reward: -533.62\n",
      "========= Step:  20 =========\n",
      "State:          [ 19 -34 -35 -32   2   1   1   2   1   2]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -197.0\n",
      "Next state:     [ 20 -36 -35 -32   2   1   2   2   1   1]\n",
      "Episode reward: -557.57\n",
      "========= Step:  21 =========\n",
      "State:          [ 20 -36 -35 -32   2   1   2   2   1   1]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -193.5\n",
      "Next state:     [ 15 -36 -36 -31   2   2   1   2   1   2]\n",
      "Episode reward: -578.74\n",
      "========= Step:  22 =========\n",
      "State:          [ 15 -36 -36 -31   2   2   1   2   1   2]\n",
      "Action:         [4 1 0 0]\n",
      "Reward:         -200.8\n",
      "Next state:     [ 18 -37 -37 -33   2   1   2   2   2   1]\n",
      "Episode reward: -598.52\n",
      "========= Step:  23 =========\n",
      "State:          [ 18 -37 -37 -33   2   1   2   2   2   1]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -201.3\n",
      "Next state:     [ 13 -36 -37 -34   1   1   3   2   1   2]\n",
      "Episode reward: -616.36\n",
      "========= Step:  24 =========\n",
      "State:          [ 13 -36 -37 -34   1   1   3   2   1   2]\n",
      "Action:         [8 0 1 0]\n",
      "Reward:         -214.0\n",
      "Next state:     [ 20 -37 -38 -36   1   2   2   1   1   3]\n",
      "Episode reward: -633.43\n",
      "========= Step:  25 =========\n",
      "State:          [ 20 -37 -38 -36   1   2   2   1   1   3]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -207.5\n",
      "Next state:     [ 15 -37 -39 -36   2   2   2   1   2   2]\n",
      "Episode reward: -648.32\n",
      "========= Step:  26 =========\n",
      "State:          [ 15 -37 -39 -36   2   2   2   1   2   2]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -213.7\n",
      "Next state:     [ 17 -38 -41 -39   2   2   4   2   2   2]\n",
      "Episode reward: -662.13\n",
      "========= Step:  27 =========\n",
      "State:          [ 17 -38 -41 -39   2   2   4   2   2   2]\n",
      "Action:         [4 1 0 0]\n",
      "Reward:         -223.0\n",
      "Next state:     [ 20 -39 -45 -42   2   4   3   2   2   4]\n",
      "Episode reward: -675.1\n",
      "========= Step:  28 =========\n",
      "State:          [ 20 -39 -45 -42   2   4   3   2   2   4]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -229.5\n",
      "Next state:     [ 15 -40 -48 -43   3   4   3   2   4   3]\n",
      "Episode reward: -687.11\n",
      "========= Step:  29 =========\n",
      "State:          [ 15 -40 -48 -43   3   4   3   2   4   3]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -244.6\n",
      "Next state:     [ 16 -44 -50 -45   4   3   4   3   4   3]\n",
      "Episode reward: -698.63\n",
      "========= Step:  30 =========\n",
      "State:          [ 16 -44 -50 -45   4   3   4   3   4   3]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -260.7\n",
      "Next state:     [ 17 -48 -53 -46   4   4   3   4   3   4]\n",
      "Episode reward: -709.68\n",
      "========= Step:  31 =========\n",
      "State:          [ 17 -48 -53 -46   4   4   3   4   3   4]\n",
      "Action:         [4 1 0 0]\n",
      "Reward:         -277.0\n",
      "Next state:     [ 20 -51 -56 -50   4   3   4   4   4   3]\n",
      "Episode reward: -720.25\n",
      "========= Step:  32 =========\n",
      "State:          [ 20 -51 -56 -50   4   3   4   4   4   3]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -291.5\n",
      "Next state:     [ 15 -53 -58 -51   4   3   3   4   3   4]\n",
      "Episode reward: -730.26\n",
      "========= Step:  33 =========\n",
      "State:          [ 15 -53 -58 -51   4   3   3   4   3   4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -307.9\n",
      "Next state:     [ 19 -56 -62 -55   3   4   4   4   3   3]\n",
      "Episode reward: -739.77\n",
      "========= Step:  34 =========\n",
      "State:          [ 19 -56 -62 -55   3   4   4   4   3   3]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -321.4\n",
      "Next state:     [ 14 -58 -64 -57   4   3   4   3   4   4]\n",
      "Episode reward: -748.71\n",
      "========= Step:  35 =========\n",
      "State:          [ 14 -58 -64 -57   4   3   4   3   4   4]\n",
      "Action:         [4 1 0 1]\n",
      "Reward:         -343.6\n",
      "Next state:     [ 16 -60 -68 -59   3   4   3   4   3   4]\n",
      "Episode reward: -757.31\n",
      "========= Step:  36 =========\n",
      "State:          [ 16 -60 -68 -59   3   4   3   4   3   4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -360.0\n",
      "Next state:     [ 20 -64 -71 -62   4   3   3   3   4   3]\n",
      "Episode reward: -765.42\n",
      "========= Step:  37 =========\n",
      "State:          [ 20 -64 -71 -62   4   3   3   3   4   3]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -373.5\n",
      "Next state:     [ 15 -66 -73 -62   4   3   2   4   3   3]\n",
      "Episode reward: -773.0\n",
      "========= Step:  38 =========\n",
      "State:          [ 15 -66 -73 -62   4   3   2   4   3   3]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -393.9\n",
      "Next state:     [ 19 -69 -75 -64   3   2   2   4   3   2]\n",
      "Episode reward: -780.19\n",
      "========= Step:  39 =========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:          [ 19 -69 -75 -64   3   2   2   4   3   2]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -407.0\n",
      "Next state:     [ 20 -72 -77 -63   3   3   1   3   2   2]\n",
      "Episode reward: -786.87\n",
      "========= Step:  40 =========\n",
      "State:          [ 20 -72 -77 -63   3   3   1   3   2   2]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -407.5\n",
      "Next state:     [ 15 -73 -78 -63   3   2   2   3   3   1]\n",
      "Episode reward: -792.89\n",
      "========= Step:  41 =========\n",
      "State:          [ 15 -73 -78 -63   3   2   2   3   3   1]\n",
      "Action:         [4 1 0 0]\n",
      "Reward:         -424.8\n",
      "Next state:     [ 18 -73 -79 -65   1   1   2   3   2   2]\n",
      "Episode reward: -798.54\n",
      "========= Step:  42 =========\n",
      "State:          [ 18 -73 -79 -65   1   1   2   3   2   2]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -430.9\n",
      "Next state:     [ 19 -74 -79 -65   1   1   2   1   1   2]\n",
      "Episode reward: -803.7\n",
      "========= Step:  43 =========\n",
      "State:          [ 19 -74 -79 -65   1   1   2   1   1   2]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -425.4\n",
      "Next state:     [ 14 -74 -79 -64   2   1   1   1   1   2]\n",
      "Episode reward: -808.29\n",
      "========= Step:  44 =========\n",
      "State:          [ 14 -74 -79 -64   2   1   1   1   1   2]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -426.5\n",
      "Next state:     [ 15 -76 -80 -64   2   2   2   2   1   1]\n",
      "Episode reward: -812.42\n",
      "========= Step:  45 =========\n",
      "State:          [ 15 -76 -80 -64   2   2   2   2   1   1]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -435.9\n",
      "Next state:     [ 19 -78 -81 -66   2   1   2   2   2   2]\n",
      "Episode reward: -816.23\n",
      "========= Step:  46 =========\n",
      "State:          [ 19 -78 -81 -66   2   1   2   2   2   2]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -445.0\n",
      "Next state:     [ 20 -80 -82 -65   2   2   1   2   1   2]\n",
      "Episode reward: -819.72\n",
      "========= Step:  47 =========\n",
      "State:          [ 20 -80 -82 -65   2   2   1   2   1   2]\n",
      "Action:         [0 0 2 1]\n",
      "Reward:         -444.7\n",
      "Next state:     [ 17 -81 -82 -66   1   2   2   2   2   1]\n",
      "Episode reward: -822.87\n",
      "========= Step:  48 =========\n",
      "State:          [ 17 -81 -82 -66   1   2   2   2   2   1]\n",
      "Action:         [4 0 1 2]\n",
      "Reward:         -452.8\n",
      "Next state:     [ 18 -82 -83 -66   1   2   2   1   2   2]\n",
      "Episode reward: -825.75\n",
      "========= Step:  49 =========\n",
      "State:          [ 18 -82 -83 -66   1   2   2   1   2   2]\n",
      "Action:         [4 0 1 1]\n",
      "Reward:         -459.0\n",
      "Next state:     [ 20 -84 -83 -67   2   1   2   1   2   2]\n",
      "Episode reward: -828.38\n",
      "========= Step:  50 =========\n",
      "State:          [ 20 -84 -83 -67   2   1   2   1   2   2]\n",
      "Action:         [0 2 1 2]\n",
      "Reward:         -453.5\n",
      "Next state:     [ 15 -83 -85 -67   1   3   2   2   1   2]\n",
      "Episode reward: -830.71\n",
      "Algorithm time:  6.003042221069336  seconds!\n",
      "timeforallowedact =  2.9015958309173584\n",
      "time for update =  0.058153390884399414\n",
      "========= Step:  51 =========\n",
      "State:          [ 15 -83 -85 -67   1   3   2   2   1   2]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -457.9\n",
      "Next state:     [ 19 -86 -88 -70   3   3   3   1   3   2]\n",
      "Episode reward: -832.84\n",
      "Episode  2\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0 0 0 0]\n",
      "Action:         [8 0 2 0]\n",
      "Reward:         -12.1\n",
      "Next state:     [11  0  2  0  0  0  0  0  0  0]\n",
      "Episode reward: -12.1\n",
      "========= Step:   1 =========\n",
      "State:          [11  0  2  0  0  0  0  0  0  0]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         26.0\n",
      "Next state:     [10 -2 -2 -4  3  4  4  0  0  0]\n",
      "Episode reward: 11.3\n",
      "========= Step:   2 =========\n",
      "State:          [10 -2 -2 -4  3  4  4  0  0  0]\n",
      "Action:         [4 2 0 2]\n",
      "Reward:         5.0\n",
      "Next state:     [10 -3 -6 -6  3  4  4  3  4  4]\n",
      "Episode reward: 15.35\n",
      "========= Step:   3 =========\n",
      "State:          [10 -3 -6 -6  3  4  4  3  4  4]\n",
      "Action:         [8 1 0 1]\n",
      "Reward:         -17.6\n",
      "Next state:     [ 16  -6 -10  -8   4   4   3   3   4   4]\n",
      "Episode reward: 2.52\n",
      "========= Step:   4 =========\n",
      "State:          [ 16  -6 -10  -8   4   4   3   3   4   4]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -23.4\n",
      "Next state:     [ 14 -10 -12 -12   4   4   4   4   4   3]\n",
      "Episode reward: -12.83\n",
      "========= Step:   5 =========\n",
      "State:          [ 14 -10 -12 -12   4   4   4   4   4   3]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -48.1\n",
      "Next state:     [ 11 -14 -14 -13   4   3   3   4   4   4]\n",
      "Episode reward: -41.24\n",
      "========= Step:   6 =========\n",
      "State:          [ 11 -14 -14 -13   4   3   3   4   4   4]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -64.0\n",
      "Next state:     [ 10 -17 -17 -16   3   3   4   4   3   3]\n",
      "Episode reward: -75.25\n",
      "========= Step:   7 =========\n",
      "State:          [ 10 -17 -17 -16   3   3   4   4   3   3]\n",
      "Action:         [4 2 2 2]\n",
      "Reward:         -80.8\n",
      "Next state:     [  8 -18 -18 -17   3   3   3   3   3   4]\n",
      "Episode reward: -113.89\n",
      "========= Step:   8 =========\n",
      "State:          [  8 -18 -18 -17   3   3   3   3   3   4]\n",
      "Action:         [8 0 2 1]\n",
      "Reward:         -92.3\n",
      "Next state:     [ 13 -22 -20 -19   4   4   3   3   3   3]\n",
      "Episode reward: -153.63\n",
      "========= Step:   9 =========\n",
      "State:          [ 13 -22 -20 -19   4   4   3   3   3   3]\n",
      "Action:         [4 2 2 2]\n",
      "Reward:         -99.1\n",
      "Next state:     [ 11 -23 -22 -21   3   4   4   4   4   3]\n",
      "Episode reward: -192.02\n",
      "========= Step:  10 =========\n",
      "State:          [ 11 -23 -22 -21   3   4   4   4   4   3]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -122.7\n",
      "Next state:     [ 17 -27 -25 -21   4   3   2   3   4   4]\n",
      "Episode reward: -234.8\n",
      "========= Step:  11 =========\n",
      "State:          [ 17 -27 -25 -21   4   3   2   3   4   4]\n",
      "Action:         [4 2 2 2]\n",
      "Reward:         -129.5\n",
      "Next state:     [ 15 -28 -26 -21   3   3   2   4   3   2]\n",
      "Episode reward: -275.44\n",
      "========= Step:  12 =========\n",
      "State:          [ 15 -28 -26 -21   3   3   2   4   3   2]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -133.3\n",
      "Next state:     [ 13 -30 -27 -24   2   3   3   3   3   2]\n",
      "Episode reward: -313.09\n",
      "========= Step:  13 =========\n",
      "State:          [ 13 -30 -27 -24   2   3   3   3   3   2]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -150.0\n",
      "Next state:     [ 10 -32 -29 -23   2   3   1   2   3   3]\n",
      "Episode reward: -351.22\n",
      "========= Step:  14 =========\n",
      "State:          [ 10 -32 -29 -23   2   3   1   2   3   3]\n",
      "Action:         [4 2 2 2]\n",
      "Reward:         -158.8\n",
      "Next state:     [  8 -32 -28 -22   2   1   1   2   3   1]\n",
      "Episode reward: -387.55\n",
      "========= Step:  15 =========\n",
      "State:          [  8 -32 -28 -22   2   1   1   2   3   1]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -164.4\n",
      "Next state:     [ 14 -33 -29 -22   1   1   2   2   1   1]\n",
      "Episode reward: -421.39\n",
      "========= Step:  16 =========\n",
      "State:          [ 14 -33 -29 -22   1   1   2   2   1   1]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -167.0\n",
      "Next state:     [ 20 -35 -31 -21   2   2   1   1   1   2]\n",
      "Episode reward: -452.34\n",
      "========= Step:  17 =========\n",
      "State:          [ 20 -35 -31 -21   2   2   1   1   1   2]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -165.8\n",
      "Next state:     [ 18 -36 -30 -23   1   1   2   2   2   1]\n",
      "Episode reward: -479.99\n",
      "========= Step:  18 =========\n",
      "State:          [ 18 -36 -30 -23   1   1   2   2   2   1]\n",
      "Action:         [4 2 2 2]\n",
      "Reward:         -169.6\n",
      "Next state:     [ 16 -35 -29 -23   1   1   2   1   1   2]\n",
      "Episode reward: -505.45\n",
      "========= Step:  19 =========\n",
      "State:          [ 16 -35 -29 -23   1   1   2   1   1   2]\n",
      "Action:         [4 1 2 0]\n",
      "Reward:         -166.7\n",
      "Next state:     [ 17 -36 -29 -24   2   2   1   1   1   2]\n",
      "Episode reward: -527.96\n",
      "========= Step:  20 =========\n",
      "State:          [ 17 -36 -29 -24   2   2   1   1   1   2]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -167.5\n",
      "Next state:     [ 15 -38 -28 -26   2   1   2   2   2   1]\n",
      "Episode reward: -548.33\n",
      "========= Step:  21 =========\n",
      "State:          [ 15 -38 -28 -26   2   1   2   2   2   1]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -173.3\n",
      "Next state:     [ 13 -40 -28 -27   2   2   1   2   1   2]\n",
      "Episode reward: -567.29\n",
      "========= Step:  22 =========\n",
      "State:          [ 13 -40 -28 -27   2   2   1   2   1   2]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -190.9\n",
      "Next state:     [ 19 -41 -30 -26   1   2   1   2   2   1]\n",
      "Episode reward: -586.09\n",
      "========= Step:  23 =========\n",
      "State:          [ 19 -41 -30 -26   1   2   1   2   2   1]\n",
      "Action:         [0 2 2 0]\n",
      "Reward:         -180.5\n",
      "Next state:     [ 15 -41 -29 -28   2   1   2   1   2   1]\n",
      "Episode reward: -602.09\n",
      "========= Step:  24 =========\n",
      "State:          [ 15 -41 -29 -28   2   1   2   1   2   1]\n",
      "Action:         [0 2 2 0]\n",
      "Reward:         -180.1\n",
      "Next state:     [ 11 -40 -30 -30   1   3   2   2   1   2]\n",
      "Episode reward: -616.45\n",
      "========= Step:  25 =========\n",
      "State:          [ 11 -40 -30 -30   1   3   2   2   1   2]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -196.7\n",
      "Next state:     [ 17 -42 -32 -30   2   2   2   1   3   2]\n",
      "Episode reward: -630.58\n",
      "========= Step:  26 =========\n",
      "State:          [ 17 -42 -32 -30   2   2   2   1   3   2]\n",
      "Action:         [4 1 2 0]\n",
      "Reward:         -192.8\n",
      "Next state:     [ 18 -43 -33 -34   2   3   4   2   2   2]\n",
      "Episode reward: -643.03\n",
      "========= Step:  27 =========\n",
      "State:          [ 18 -43 -33 -34   2   3   4   2   2   2]\n",
      "Action:         [4 1 2 0]\n",
      "Reward:         -204.9\n",
      "Next state:     [ 19 -44 -34 -38   2   3   4   2   3   4]\n",
      "Episode reward: -654.95\n",
      "========= Step:  28 =========\n",
      "State:          [ 19 -44 -34 -38   2   3   4   2   3   4]\n",
      "Action:         [0 2 2 0]\n",
      "Reward:         -208.5\n",
      "Next state:     [ 15 -45 -36 -41   3   4   3   2   3   4]\n",
      "Episode reward: -665.86\n",
      "========= Step:  29 =========\n",
      "State:          [ 15 -45 -36 -41   3   4   3   2   3   4]\n",
      "Action:         [8 2 0 2]\n",
      "Reward:         -225.9\n",
      "Next state:     [ 19 -47 -40 -43   4   4   4   3   4   3]\n",
      "Episode reward: -676.5\n",
      "========= Step:  30 =========\n",
      "State:          [ 19 -47 -40 -43   4   4   4   3   4   3]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -237.7\n",
      "Next state:     [ 17 -51 -41 -47   4   3   4   4   4   4]\n",
      "Episode reward: -686.58\n",
      "========= Step:  31 =========\n",
      "State:          [ 17 -51 -41 -47   4   3   4   4   4   4]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -255.5\n",
      "Next state:     [ 15 -55 -42 -51   4   3   4   4   3   4]\n",
      "Episode reward: -696.32\n",
      "========= Step:  32 =========\n",
      "State:          [ 15 -55 -42 -51   4   3   4   4   3   4]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -271.3\n",
      "Next state:     [ 13 -59 -44 -55   4   4   4   4   3   4]\n",
      "Episode reward: -705.64\n",
      "========= Step:  33 =========\n",
      "State:          [ 13 -59 -44 -55   4   4   4   4   3   4]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -302.9\n",
      "Next state:     [ 19 -63 -48 -56   4   4   3   4   4   4]\n",
      "Episode reward: -715.0\n",
      "========= Step:  34 =========\n",
      "State:          [ 19 -63 -48 -56   4   4   3   4   4   4]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -313.7\n",
      "Next state:     [ 17 -66 -50 -59   3   4   3   4   4   3]\n",
      "Episode reward: -723.72\n",
      "========= Step:  35 =========\n",
      "State:          [ 17 -66 -50 -59   3   4   3   4   4   3]\n",
      "Action:         [4 1 2 0]\n",
      "Reward:         -328.8\n",
      "Next state:     [ 18 -69 -52 -63   4   4   4   3   4   3]\n",
      "Episode reward: -731.95\n",
      "========= Step:  36 =========\n",
      "State:          [ 18 -69 -52 -63   4   4   4   3   4   3]\n",
      "Action:         [0 2 2 0]\n",
      "Reward:         -348.4\n",
      "Next state:     [ 14 -70 -53 -65   3   3   2   4   4   4]\n",
      "Episode reward: -739.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\evaluate_agent.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Select a new action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0maction_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Update episode reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mallowed_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeforallowedact\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py\u001b[0m in \u001b[0;36mallowed_actions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpointer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mpointer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mpointer1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpointer2\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscrete2continuous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mnp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpointer1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mnp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscrete2continuous\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpointer2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run evaluate_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJKEYSBJwjCQJGEYSJIwDCRJwP8DJ83wPGyKhVoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c02f1eada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhoDQZIEwP8BIxu53zYA/+UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c02c7645f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-900.04168844 -832.83761236    0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.            0.\n",
      "    0.            0.            0.            0.            0.            0.\n",
      "    0.            0.        ]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "plt.hist(rewards, normed=True)\n",
    "plt.show()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
