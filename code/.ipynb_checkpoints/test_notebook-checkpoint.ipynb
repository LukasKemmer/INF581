{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run evaluate_agent2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actions:  81\n",
      "Episode  0\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [8 1 2 1]\n",
      "Reward:         -17.49\n",
      "Next state:     [9 1 2 1 0 0 0]\n",
      "Episode reward: -17.49\n",
      "========= Step:   1 =========\n",
      "State:          [9 1 2 1 0 0 0]\n",
      "Action:         [8 1 0 2]\n",
      "Reward:         32.86\n",
      "Next state:     [14  0 -2 -1  2  4  4]\n",
      "Episode reward: 12.08\n",
      "========= Step:   2 =========\n",
      "State:          [14  0 -2 -1  2  4  4]\n",
      "Action:         [0 1 1 0]\n",
      "Reward:         29.88\n",
      "Next state:     [12 -2 -4 -4  3  3  3]\n",
      "Episode reward: 36.29\n",
      "========= Step:   3 =========\n",
      "State:          [12 -2 -4 -4  3  3  3]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         25.8\n",
      "Next state:     [20 -5 -8 -8  3  4  4]\n",
      "Episode reward: 55.1\n",
      "========= Step:   4 =========\n",
      "State:          [20 -5 -8 -8  3  4  4]\n",
      "Action:         [0 1 2 2]\n",
      "Reward:         10.85\n",
      "Next state:     [15 -7 -9 -9  3  3  3]\n",
      "Episode reward: 62.21\n",
      "========= Step:   5 =========\n",
      "State:          [15 -7 -9 -9  3  3  3]\n",
      "Action:         [4 0 1 1]\n",
      "Reward:         1.83\n",
      "Next state:     [ 17 -10 -11 -11   3   3   3]\n",
      "Episode reward: 63.29\n",
      "========= Step:   6 =========\n",
      "State:          [ 17 -10 -11 -11   3   3   3]\n",
      "Action:         [0 1 2 1]\n",
      "Reward:         -5.13\n",
      "Next state:     [ 13 -12 -13 -11   3   4   1]\n",
      "Episode reward: 60.57\n",
      "========= Step:   7 =========\n",
      "State:          [ 13 -12 -13 -11   3   4   1]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -25.19\n",
      "Next state:     [ 19 -15 -14 -11   3   2   1]\n",
      "Episode reward: 48.52\n",
      "========= Step:   8 =========\n",
      "State:          [ 19 -15 -14 -11   3   2   1]\n",
      "Action:         [0 1 2 1]\n",
      "Reward:         -17.15\n",
      "Next state:     [ 15 -18 -14 -11   4   2   1]\n",
      "Episode reward: 41.14\n",
      "========= Step:   9 =========\n",
      "State:          [ 15 -18 -14 -11   4   2   1]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -14.12\n",
      "Next state:     [ 12 -20 -16 -12   4   2   2]\n",
      "Episode reward: 35.67\n",
      "========= Step:  10 =========\n",
      "State:          [ 12 -20 -16 -12   4   2   2]\n",
      "Action:         [4 0 1 1]\n",
      "Reward:         -29.14\n",
      "Next state:     [ 14 -23 -17 -13   3   2   2]\n",
      "Episode reward: 25.51\n",
      "========= Step:  11 =========\n",
      "State:          [ 14 -23 -17 -13   3   2   2]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -42.2\n",
      "Next state:     [ 20 -25 -18 -14   2   2   2]\n",
      "Episode reward: 12.26\n",
      "========= Step:  12 =========\n",
      "State:          [ 20 -25 -18 -14   2   2   2]\n",
      "Action:         [0 1 1 1]\n",
      "Reward:         -35.17\n",
      "Next state:     [ 17 -27 -19 -15   3   2   2]\n",
      "Episode reward: 2.33\n",
      "========= Step:  13 =========\n",
      "State:          [ 17 -27 -19 -15   3   2   2]\n",
      "Action:         [0 1 1 1]\n",
      "Reward:         -51.14\n",
      "Next state:     [ 14 -28 -19 -15   2   1   1]\n",
      "Episode reward: -10.67\n",
      "========= Step:  14 =========\n",
      "State:          [ 14 -28 -19 -15   2   1   1]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -55.2\n",
      "Next state:     [ 20 -29 -20 -16   1   2   2]\n",
      "Episode reward: -23.3\n",
      "========= Step:  15 =========\n",
      "State:          [ 20 -29 -20 -16   1   2   2]\n",
      "Action:         [0 2 1 0]\n",
      "Reward:         -55.17\n",
      "Next state:     [ 17 -28 -20 -17   1   1   1]\n",
      "Episode reward: -34.66\n",
      "========= Step:  16 =========\n",
      "State:          [ 17 -28 -20 -17   1   1   1]\n",
      "Action:         [0 1 1 1]\n",
      "Reward:         -55.14\n",
      "Next state:     [ 14 -28 -20 -18   1   1   2]\n",
      "Episode reward: -44.87\n",
      "========= Step:  17 =========\n",
      "State:          [ 14 -28 -20 -18   1   1   2]\n",
      "Action:         [0 0 2 2]\n",
      "Reward:         -49.1\n",
      "Next state:     [ 10 -29 -20 -18   1   2   2]\n",
      "Episode reward: -53.06\n",
      "========= Step:  18 =========\n",
      "State:          [ 10 -29 -20 -18   1   2   2]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -56.16\n",
      "Next state:     [ 16 -30 -21 -20   1   2   3]\n",
      "Episode reward: -61.49\n",
      "========= Step:  19 =========\n",
      "State:          [ 16 -30 -21 -20   1   2   3]\n",
      "Action:         [0 1 2 1]\n",
      "Reward:         -44.12\n",
      "Next state:     [ 12 -30 -22 -23   1   3   4]\n",
      "Episode reward: -67.45\n",
      "========= Step:  20 =========\n",
      "State:          [ 12 -30 -22 -23   1   3   4]\n",
      "Action:         [8 0 1 1]\n",
      "Reward:         -56.18\n",
      "Next state:     [ 18 -31 -24 -26   1   3   4]\n",
      "Episode reward: -74.28\n",
      "========= Step:  21 =========\n",
      "State:          [ 18 -31 -24 -26   1   3   4]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -52.15\n",
      "Next state:     [ 15 -30 -27 -29   1   3   4]\n",
      "Episode reward: -79.99\n",
      "========= Step:  22 =========\n",
      "State:          [ 15 -30 -27 -29   1   3   4]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -53.12\n",
      "Next state:     [ 12 -30 -30 -32   2   3   4]\n",
      "Episode reward: -85.22\n",
      "Update: [[ 70.     5.     0.     0.     0.     0.     0.     0.   -17.49]\n",
      " [ 65.     9.     1.     2.     1.     0.     0.     0.    32.86]\n",
      " [ 12.    14.     0.    -2.    -1.     2.     4.     4.    29.88]\n",
      " [ 54.    12.    -2.    -4.    -4.     3.     3.     3.    25.8 ]\n",
      " [ 17.    20.    -5.    -8.    -8.     3.     4.     4.    10.85]\n",
      " [ 31.    15.    -7.    -9.    -9.     3.     3.     3.     1.83]\n",
      " [ 16.    17.   -10.   -11.   -11.     3.     3.     3.    -5.13]\n",
      " [ 58.    13.   -12.   -13.   -11.     3.     4.     1.   -25.19]\n",
      " [ 16.    19.   -15.   -14.   -11.     3.     2.     1.   -17.15]\n",
      " [ 19.    15.   -18.   -14.   -11.     4.     2.     1.   -14.12]\n",
      " [ 31.    12.   -20.   -16.   -12.     4.     2.     2.   -29.14]\n",
      " [ 58.    14.   -23.   -17.   -13.     3.     2.     2.   -42.2 ]\n",
      " [ 13.    20.   -25.   -18.   -14.     2.     2.     2.   -35.17]\n",
      " [ 13.    17.   -27.   -19.   -15.     3.     2.     2.   -51.14]\n",
      " [ 58.    14.   -28.   -19.   -15.     2.     1.     1.   -55.2 ]\n",
      " [ 21.    20.   -29.   -20.   -16.     1.     2.     2.   -55.17]\n",
      " [ 13.    17.   -28.   -20.   -17.     1.     1.     1.   -55.14]\n",
      " [  8.    14.   -28.   -20.   -18.     1.     1.     2.   -49.1 ]\n",
      " [ 58.    10.   -29.   -20.   -18.     1.     2.     2.   -56.16]\n",
      " [ 16.    16.   -30.   -21.   -20.     1.     2.     3.   -44.12]\n",
      " [ 58.    12.   -30.   -22.   -23.     1.     3.     4.   -56.18]\n",
      " [ 19.    18.   -31.   -24.   -26.     1.     3.     4.   -52.15]\n",
      " [ 19.    15.   -30.   -27.   -29.     1.     3.     4.   -53.12]\n",
      " [ 19.    12.   -30.   -30.   -32.     2.     3.     4.   -55.09]\n",
      " [ 19.     9.   -31.   -34.   -34.     3.     4.     3.     0.  ]]\n",
      "========= Step:  23 =========\n",
      "State:          [ 12 -30 -30 -32   2   3   4]\n",
      "Action:         [0 2 0 1]\n",
      "Reward:         -55.09\n",
      "Next state:     [  9 -31 -34 -34   3   4   3]\n",
      "Episode reward: -90.1\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [4 2 2 0]\n",
      "Reward:         -9.45\n",
      "Next state:     [5 2 2 0 0 0 0]\n",
      "Episode reward: -9.45\n",
      "========= Step:   1 =========\n",
      "State:          [5 2 2 0 0 0 0]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         31.88\n",
      "Next state:     [ 2  1 -1 -1  2  3  3]\n",
      "Episode reward: 19.24\n",
      "========= Step:   2 =========\n",
      "State:          [ 2  1 -1 -1  2  3  3]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         42.0\n",
      "Next state:     [ 0 -3 -3 -4  4  4  3]\n",
      "Episode reward: 53.26\n",
      "========= Step:   3 =========\n",
      "State:          [ 0 -3 -3 -4  4  4  3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         38.0\n",
      "Next state:     [ 0 -7 -7 -8  4  4  4]\n",
      "Episode reward: 80.96\n",
      "========= Step:   4 =========\n",
      "State:          [ 0 -7 -7 -8  4  4  4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         17.96\n",
      "Next state:     [  4 -11 -11 -11   4   4   3]\n",
      "Episode reward: 92.75\n",
      "========= Step:   5 =========\n",
      "State:          [  4 -11 -11 -11   4   4   3]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -4.03\n",
      "Next state:     [  3 -14 -14 -12   3   3   2]\n",
      "Episode reward: 90.37\n",
      "========= Step:   6 =========\n",
      "State:          [  3 -14 -14 -12   3   3   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -7.02\n",
      "Next state:     [  2 -18 -17 -13   4   3   2]\n",
      "Episode reward: 86.64\n",
      "========= Step:   7 =========\n",
      "State:          [  2 -18 -17 -13   4   3   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -23.01\n",
      "Next state:     [  1 -22 -19 -13   4   2   1]\n",
      "Episode reward: 75.63\n",
      "========= Step:   8 =========\n",
      "State:          [  1 -22 -19 -13   4   2   1]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -29.0\n",
      "Next state:     [  0 -26 -21 -13   4   2   1]\n",
      "Episode reward: 63.15\n",
      "========= Step:   9 =========\n",
      "State:          [  0 -26 -21 -13   4   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -28.0\n",
      "Next state:     [  0 -30 -24 -14   4   3   1]\n",
      "Episode reward: 52.3\n",
      "========= Step:  10 =========\n",
      "State:          [  0 -30 -24 -14   4   3   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -36.0\n",
      "Next state:     [  0 -34 -26 -16   4   2   2]\n",
      "Episode reward: 39.75\n",
      "========= Step:  11 =========\n",
      "State:          [  0 -34 -26 -16   4   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -52.0\n",
      "Next state:     [  0 -37 -28 -17   3   2   1]\n",
      "Episode reward: 23.43\n",
      "========= Step:  12 =========\n",
      "State:          [  0 -37 -28 -17   3   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -54.0\n",
      "Next state:     [  0 -40 -30 -19   3   2   2]\n",
      "Episode reward: 8.18\n",
      "========= Step:  13 =========\n",
      "State:          [  0 -40 -30 -19   3   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -65.0\n",
      "Next state:     [  0 -43 -31 -21   3   1   2]\n",
      "Episode reward: -8.34\n",
      "========= Step:  14 =========\n",
      "State:          [  0 -43 -31 -21   3   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -83.0\n",
      "Next state:     [  0 -44 -32 -22   1   1   1]\n",
      "Episode reward: -27.33\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -44 -32 -22   1   1   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -78.0\n",
      "Next state:     [  0 -46 -34 -23   2   2   1]\n",
      "Episode reward: -43.39\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -46 -34 -23   2   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -75.0\n",
      "Next state:     [  0 -48 -36 -26   2   2   3]\n",
      "Episode reward: -57.29\n",
      "========= Step:  17 =========\n",
      "State:          [  0 -48 -36 -26   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -86.0\n",
      "Next state:     [  0 -49 -38 -29   1   2   3]\n",
      "Episode reward: -71.63\n",
      "========= Step:  18 =========\n",
      "State:          [  0 -49 -38 -29   1   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -92.0\n",
      "Next state:     [  0 -50 -39 -33   1   1   4]\n",
      "Episode reward: -85.44\n",
      "========= Step:  19 =========\n",
      "State:          [  0 -50 -39 -33   1   1   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -94.0\n",
      "Next state:     [  0 -52 -41 -36   2   2   3]\n",
      "Episode reward: -98.14\n",
      "========= Step:  20 =========\n",
      "State:          [  0 -52 -41 -36   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -97.0\n",
      "Next state:     [  0 -53 -44 -40   1   3   4]\n",
      "Episode reward: -109.93\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -53 -44 -40   1   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -109.0\n",
      "Next state:     [  0 -55 -46 -43   2   2   3]\n",
      "Episode reward: -121.86\n",
      "========= Step:  22 =========\n",
      "State:          [  0 -55 -46 -43   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -112.0\n",
      "Next state:     [  0 -56 -50 -46   1   4   3]\n",
      "Episode reward: -132.89\n",
      "Update: [[  51.      5.      0.      0.      0.      0.      0.      0.     -9.45]\n",
      " [  11.      5.      2.      2.      0.      0.      0.      0.     31.88]\n",
      " [   6.      2.      1.     -1.     -1.      2.      3.      3.     42.  ]\n",
      " [   0.      0.     -3.     -3.     -4.      4.      4.      3.     38.  ]\n",
      " [  27.      0.     -7.     -7.     -8.      4.      4.      4.     17.96]\n",
      " [   1.      4.    -11.    -11.    -11.      4.      4.      3.     -4.03]\n",
      " [   1.      3.    -14.    -14.    -12.      3.      3.      2.     -7.02]\n",
      " [   1.      2.    -18.    -17.    -13.      4.      3.      2.    -23.01]\n",
      " [   1.      1.    -22.    -19.    -13.      4.      2.      1.    -29.  ]\n",
      " [   0.      0.    -26.    -21.    -13.      4.      2.      1.    -28.  ]\n",
      " [   0.      0.    -30.    -24.    -14.      4.      3.      1.    -36.  ]\n",
      " [   0.      0.    -34.    -26.    -16.      4.      2.      2.    -52.  ]\n",
      " [   0.      0.    -37.    -28.    -17.      3.      2.      1.    -54.  ]\n",
      " [   0.      0.    -40.    -30.    -19.      3.      2.      2.    -65.  ]\n",
      " [   0.      0.    -43.    -31.    -21.      3.      1.      2.    -83.  ]\n",
      " [   0.      0.    -44.    -32.    -22.      1.      1.      1.    -78.  ]\n",
      " [   0.      0.    -46.    -34.    -23.      2.      2.      1.    -75.  ]\n",
      " [   0.      0.    -48.    -36.    -26.      2.      2.      3.    -86.  ]\n",
      " [   0.      0.    -49.    -38.    -29.      1.      2.      3.    -92.  ]\n",
      " [   0.      0.    -50.    -39.    -33.      1.      1.      4.    -94.  ]\n",
      " [   0.      0.    -52.    -41.    -36.      2.      2.      3.    -97.  ]\n",
      " [   0.      0.    -53.    -44.    -40.      1.      3.      4.   -109.  ]\n",
      " [   0.      0.    -55.    -46.    -43.      2.      2.      3.   -112.  ]\n",
      " [   0.      0.    -56.    -50.    -46.      1.      4.      3.   -120.  ]\n",
      " [   0.      0.    -58.    -53.    -49.      2.      3.      3.      0.  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  23 =========\n",
      "State:          [  0 -56 -50 -46   1   4   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -120.0\n",
      "Next state:     [  0 -58 -53 -49   2   3   3]\n",
      "Episode reward: -143.52\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -0.05\n",
      "Next state:     [5 0 0 0 0 0 0]\n",
      "Episode reward: -0.05\n",
      "========= Step:   1 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 1 2 0]\n",
      "Reward:         41.98\n",
      "Next state:     [ 2 -2 -2 -4  3  4  4]\n",
      "Episode reward: 37.73\n",
      "========= Step:   2 =========\n",
      "State:          [ 2 -2 -2 -4  3  4  4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         31.94\n",
      "Next state:     [ 6 -6 -6 -7  4  4  3]\n",
      "Episode reward: 63.6\n",
      "========= Step:   3 =========\n",
      "State:          [ 6 -6 -6 -7  4  4  3]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         20.9\n",
      "Next state:     [ 10 -10 -10 -10   4   4   3]\n",
      "Episode reward: 78.84\n",
      "========= Step:   4 =========\n",
      "State:          [ 10 -10 -10 -10   4   4   3]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         5.86\n",
      "Next state:     [ 14 -14 -14 -12   4   4   2]\n",
      "Episode reward: 82.68\n",
      "========= Step:   5 =========\n",
      "State:          [ 14 -14 -14 -12   4   4   2]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -8.18\n",
      "Next state:     [ 18 -17 -18 -14   3   4   2]\n",
      "Episode reward: 77.85\n",
      "========= Step:   6 =========\n",
      "State:          [ 18 -17 -18 -14   3   4   2]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -14.17\n",
      "Next state:     [ 17 -19 -22 -16   3   4   2]\n",
      "Episode reward: 70.32\n",
      "========= Step:   7 =========\n",
      "State:          [ 17 -19 -22 -16   3   4   2]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -22.16\n",
      "Next state:     [ 16 -22 -25 -18   4   3   2]\n",
      "Episode reward: 59.72\n",
      "========= Step:   8 =========\n",
      "State:          [ 16 -22 -25 -18   4   3   2]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -37.2\n",
      "Next state:     [ 20 -26 -28 -19   4   3   1]\n",
      "Episode reward: 43.71\n",
      "========= Step:   9 =========\n",
      "State:          [ 20 -26 -28 -19   4   3   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -50.19\n",
      "Next state:     [ 19 -28 -30 -20   3   2   1]\n",
      "Episode reward: 24.27\n",
      "========= Step:  10 =========\n",
      "State:          [ 19 -28 -30 -20   3   2   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -55.18\n",
      "Next state:     [ 18 -31 -31 -21   4   1   1]\n",
      "Episode reward: 5.03\n",
      "========= Step:  11 =========\n",
      "State:          [ 18 -31 -31 -21   4   1   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -56.17\n",
      "Next state:     [ 17 -33 -33 -23   3   2   2]\n",
      "Episode reward: -12.6\n",
      "========= Step:  12 =========\n",
      "State:          [ 17 -33 -33 -23   3   2   2]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -70.16\n",
      "Next state:     [ 16 -34 -34 -25   2   1   2]\n",
      "Episode reward: -32.42\n",
      "========= Step:  13 =========\n",
      "State:          [ 16 -34 -34 -25   2   1   2]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -81.2\n",
      "Next state:     [ 20 -36 -35 -26   2   1   1]\n",
      "Episode reward: -53.06\n",
      "========= Step:  14 =========\n",
      "State:          [ 20 -36 -35 -26   2   1   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -86.19\n",
      "Next state:     [ 19 -36 -36 -27   1   1   1]\n",
      "Episode reward: -72.77\n",
      "========= Step:  15 =========\n",
      "State:          [ 19 -36 -36 -27   1   1   1]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -80.18\n",
      "Next state:     [ 18 -37 -37 -29   2   1   2]\n",
      "Episode reward: -89.28\n",
      "========= Step:  16 =========\n",
      "State:          [ 18 -37 -37 -29   2   1   2]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -84.17\n",
      "Next state:     [ 17 -38 -38 -31   2   1   2]\n",
      "Episode reward: -104.88\n",
      "========= Step:  17 =========\n",
      "State:          [ 17 -38 -38 -31   2   1   2]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -88.16\n",
      "Next state:     [ 16 -39 -39 -33   2   1   2]\n",
      "Episode reward: -119.58\n",
      "========= Step:  18 =========\n",
      "State:          [ 16 -39 -39 -33   2   1   2]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -95.2\n",
      "Next state:     [ 20 -39 -40 -37   0   1   4]\n",
      "Episode reward: -133.87\n",
      "========= Step:  19 =========\n",
      "State:          [ 20 -39 -40 -37   0   1   4]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -85.19\n",
      "Next state:     [ 19 -40 -43 -40   2   3   3]\n",
      "Episode reward: -145.38\n",
      "========= Step:  20 =========\n",
      "State:          [ 19 -40 -43 -40   2   3   3]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -92.18\n",
      "Next state:     [ 18 -40 -46 -44   1   3   4]\n",
      "Episode reward: -156.58\n",
      "========= Step:  21 =========\n",
      "State:          [ 18 -40 -46 -44   1   3   4]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -103.17\n",
      "Next state:     [ 17 -40 -49 -47   1   3   3]\n",
      "Episode reward: -167.87\n",
      "========= Step:  22 =========\n",
      "State:          [ 17 -40 -49 -47   1   3   3]\n",
      "Action:         [0 1 0 0]\n",
      "Reward:         -105.16\n",
      "Next state:     [ 16 -40 -52 -51   1   3   4]\n",
      "Episode reward: -178.23\n",
      "Update: [[  0.00000000e+00   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -4.99999989e-02]\n",
      " [  1.50000000e+01   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    4.19800000e+01]\n",
      " [  2.70000000e+01   2.00000000e+00  -2.00000000e+00  -2.00000000e+00\n",
      "   -4.00000000e+00   3.00000000e+00   4.00000000e+00   4.00000000e+00\n",
      "    3.19400000e+01]\n",
      " [  2.70000000e+01   6.00000000e+00  -6.00000000e+00  -6.00000000e+00\n",
      "   -7.00000000e+00   4.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    2.09000000e+01]\n",
      " [  2.70000000e+01   1.00000000e+01  -1.00000000e+01  -1.00000000e+01\n",
      "   -1.00000000e+01   4.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    5.86000000e+00]\n",
      " [  2.70000000e+01   1.40000000e+01  -1.40000000e+01  -1.40000000e+01\n",
      "   -1.20000000e+01   4.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   -8.18000000e+00]\n",
      " [  9.00000000e+00   1.80000000e+01  -1.70000000e+01  -1.80000000e+01\n",
      "   -1.40000000e+01   3.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   -1.41700000e+01]\n",
      " [  9.00000000e+00   1.70000000e+01  -1.90000000e+01  -2.20000000e+01\n",
      "   -1.60000000e+01   3.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   -2.21600000e+01]\n",
      " [  2.70000000e+01   1.60000000e+01  -2.20000000e+01  -2.50000000e+01\n",
      "   -1.80000000e+01   4.00000000e+00   3.00000000e+00   2.00000000e+00\n",
      "   -3.72000000e+01]\n",
      " [  9.00000000e+00   2.00000000e+01  -2.60000000e+01  -2.80000000e+01\n",
      "   -1.90000000e+01   4.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   -5.01900000e+01]\n",
      " [  9.00000000e+00   1.90000000e+01  -2.80000000e+01  -3.00000000e+01\n",
      "   -2.00000000e+01   3.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "   -5.51800000e+01]\n",
      " [  9.00000000e+00   1.80000000e+01  -3.10000000e+01  -3.10000000e+01\n",
      "   -2.10000000e+01   4.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -5.61700000e+01]\n",
      " [  9.00000000e+00   1.70000000e+01  -3.30000000e+01  -3.30000000e+01\n",
      "   -2.30000000e+01   3.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "   -7.01600000e+01]\n",
      " [  2.70000000e+01   1.60000000e+01  -3.40000000e+01  -3.40000000e+01\n",
      "   -2.50000000e+01   2.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -8.12000000e+01]\n",
      " [  9.00000000e+00   2.00000000e+01  -3.60000000e+01  -3.50000000e+01\n",
      "   -2.60000000e+01   2.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -8.61900000e+01]\n",
      " [  9.00000000e+00   1.90000000e+01  -3.60000000e+01  -3.60000000e+01\n",
      "   -2.70000000e+01   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -8.01800000e+01]\n",
      " [  9.00000000e+00   1.80000000e+01  -3.70000000e+01  -3.70000000e+01\n",
      "   -2.90000000e+01   2.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -8.41700000e+01]\n",
      " [  9.00000000e+00   1.70000000e+01  -3.80000000e+01  -3.80000000e+01\n",
      "   -3.10000000e+01   2.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -8.81600000e+01]\n",
      " [  2.70000000e+01   1.60000000e+01  -3.90000000e+01  -3.90000000e+01\n",
      "   -3.30000000e+01   2.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -9.52000000e+01]\n",
      " [  9.00000000e+00   2.00000000e+01  -3.90000000e+01  -4.00000000e+01\n",
      "   -3.70000000e+01   0.00000000e+00   1.00000000e+00   4.00000000e+00\n",
      "   -8.51900000e+01]\n",
      " [  9.00000000e+00   1.90000000e+01  -4.00000000e+01  -4.30000000e+01\n",
      "   -4.00000000e+01   2.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "   -9.21800000e+01]\n",
      " [  9.00000000e+00   1.80000000e+01  -4.00000000e+01  -4.60000000e+01\n",
      "   -4.40000000e+01   1.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "   -1.03170000e+02]\n",
      " [  9.00000000e+00   1.70000000e+01  -4.00000000e+01  -4.90000000e+01\n",
      "   -4.70000000e+01   1.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "   -1.05160000e+02]\n",
      " [  2.70000000e+01   1.60000000e+01  -4.00000000e+01  -5.20000000e+01\n",
      "   -5.10000000e+01   1.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "   -1.07200000e+02]\n",
      " [  9.00000000e+00   2.00000000e+01  -4.30000000e+01  -5.60000000e+01\n",
      "   -5.40000000e+01   3.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  23 =========\n",
      "State:          [ 16 -40 -52 -51   1   3   4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -107.2\n",
      "Next state:     [ 20 -43 -56 -54   3   4   3]\n",
      "Episode reward: -187.73\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 1 0 1]\n",
      "Reward:         -6.23\n",
      "Next state:     [3 1 0 1 0 0 0]\n",
      "Episode reward: -6.23\n",
      "========= Step:   1 =========\n",
      "State:          [3 1 0 1 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         45.97\n",
      "Next state:     [ 3 -2 -4 -3  3  4  4]\n",
      "Episode reward: 35.14\n",
      "========= Step:   2 =========\n",
      "State:          [ 3 -2 -4 -3  3  4  4]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         28.99\n",
      "Next state:     [ 1 -6 -7 -4  4  3  3]\n",
      "Episode reward: 58.62\n",
      "========= Step:   3 =========\n",
      "State:          [ 1 -6 -7 -4  4  3  3]\n",
      "Action:         [4 0 0 1]\n",
      "Reward:         19.96\n",
      "Next state:     [  4  -9 -11  -7   3   4   4]\n",
      "Episode reward: 73.18\n",
      "========= Step:   4 =========\n",
      "State:          [  4  -9 -11  -7   3   4   4]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         10.98\n",
      "Next state:     [  2 -12 -15  -8   3   4   3]\n",
      "Episode reward: 80.38\n",
      "========= Step:   5 =========\n",
      "State:          [  2 -12 -15  -8   3   4   3]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -1.0\n",
      "Next state:     [  0 -16 -18  -8   4   3   2]\n",
      "Episode reward: 79.79\n",
      "========= Step:   6 =========\n",
      "State:          [  0 -16 -18  -8   4   3   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -18.08\n",
      "Next state:     [  8 -20 -21  -9   4   3   1]\n",
      "Episode reward: 70.18\n",
      "========= Step:   7 =========\n",
      "State:          [  8 -20 -21  -9   4   3   1]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -24.06\n",
      "Next state:     [  6 -23 -23  -9   3   2   2]\n",
      "Episode reward: 58.67\n",
      "========= Step:   8 =========\n",
      "State:          [  6 -23 -23  -9   3   2   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -21.04\n",
      "Next state:     [  4 -27 -26  -9   4   3   2]\n",
      "Episode reward: 49.62\n",
      "========= Step:   9 =========\n",
      "State:          [  4 -27 -26  -9   4   3   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -32.02\n",
      "Next state:     [  2 -31 -28  -9   4   2   2]\n",
      "Episode reward: 37.21\n",
      "========= Step:  10 =========\n",
      "State:          [  2 -31 -28  -9   4   2   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -46.0\n",
      "Next state:     [  0 -34 -30  -8   3   2   1]\n",
      "Episode reward: 21.17\n",
      "========= Step:  11 =========\n",
      "State:          [  0 -34 -30  -8   3   2   1]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -60.08\n",
      "Next state:     [  8 -37 -31  -9   3   1   1]\n",
      "Episode reward: 2.32\n",
      "========= Step:  12 =========\n",
      "State:          [  8 -37 -31  -9   3   1   1]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -55.06\n",
      "Next state:     [  6 -39 -33  -9   2   2   2]\n",
      "Episode reward: -13.23\n",
      "========= Step:  13 =========\n",
      "State:          [  6 -39 -33  -9   2   2   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -59.04\n",
      "Next state:     [  4 -41 -35  -9   2   2   2]\n",
      "Episode reward: -28.24\n",
      "========= Step:  14 =========\n",
      "State:          [  4 -41 -35  -9   2   2   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -67.02\n",
      "Next state:     [  2 -43 -36  -9   2   1   2]\n",
      "Episode reward: -43.57\n",
      "========= Step:  15 =========\n",
      "State:          [  2 -43 -36  -9   2   1   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -70.0\n",
      "Next state:     [  0 -45 -38  -8   2   2   1]\n",
      "Episode reward: -57.98\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -45 -38  -8   2   2   1]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -71.08\n",
      "Next state:     [  8 -47 -40 -11   2   2   3]\n",
      "Episode reward: -71.16\n",
      "========= Step:  17 =========\n",
      "State:          [  8 -47 -40 -11   2   2   3]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -80.06\n",
      "Next state:     [  6 -48 -42 -11   1   2   2]\n",
      "Episode reward: -84.51\n",
      "========= Step:  18 =========\n",
      "State:          [  6 -48 -42 -11   1   2   2]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -79.04\n",
      "Next state:     [  4 -49 -43 -13   1   1   4]\n",
      "Episode reward: -96.37\n",
      "========= Step:  19 =========\n",
      "State:          [  4 -49 -43 -13   1   1   4]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -79.02\n",
      "Next state:     [  2 -50 -45 -15   1   2   4]\n",
      "Episode reward: -107.05\n",
      "========= Step:  20 =========\n",
      "State:          [  2 -50 -45 -15   1   2   4]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -84.0\n",
      "Next state:     [  0 -51 -48 -16   1   3   3]\n",
      "Episode reward: -117.26\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -51 -48 -16   1   3   3]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -91.08\n",
      "Next state:     [  8 -53 -50 -20   2   2   4]\n",
      "Episode reward: -127.22\n",
      "========= Step:  22 =========\n",
      "State:          [  8 -53 -50 -20   2   2   4]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -97.06\n",
      "Next state:     [  6 -54 -53 -21   1   3   3]\n",
      "Episode reward: -136.78\n",
      "Update: [[ 10.     5.     0.     0.     0.     0.     0.     0.    -6.23]\n",
      " [  0.     3.     1.     0.     1.     0.     0.     0.    45.97]\n",
      " [  2.     3.    -2.    -4.    -3.     3.     4.     4.    28.99]\n",
      " [ 28.     1.    -6.    -7.    -4.     4.     3.     3.    19.96]\n",
      " [  2.     4.    -9.   -11.    -7.     3.     4.     4.    10.98]\n",
      " [  2.     2.   -12.   -15.    -8.     3.     4.     3.    -1.  ]\n",
      " [ 54.     0.   -16.   -18.    -8.     4.     3.     2.   -18.08]\n",
      " [  2.     8.   -20.   -21.    -9.     4.     3.     1.   -24.06]\n",
      " [  2.     6.   -23.   -23.    -9.     3.     2.     2.   -21.04]\n",
      " [  2.     4.   -27.   -26.    -9.     4.     3.     2.   -32.02]\n",
      " [  2.     2.   -31.   -28.    -9.     4.     2.     2.   -46.  ]\n",
      " [ 54.     0.   -34.   -30.    -8.     3.     2.     1.   -60.08]\n",
      " [  2.     8.   -37.   -31.    -9.     3.     1.     1.   -55.06]\n",
      " [  2.     6.   -39.   -33.    -9.     2.     2.     2.   -59.04]\n",
      " [  2.     4.   -41.   -35.    -9.     2.     2.     2.   -67.02]\n",
      " [  2.     2.   -43.   -36.    -9.     2.     1.     2.   -70.  ]\n",
      " [ 54.     0.   -45.   -38.    -8.     2.     2.     1.   -71.08]\n",
      " [  2.     8.   -47.   -40.   -11.     2.     2.     3.   -80.06]\n",
      " [  2.     6.   -48.   -42.   -11.     1.     2.     2.   -79.04]\n",
      " [  2.     4.   -49.   -43.   -13.     1.     1.     4.   -79.02]\n",
      " [  2.     2.   -50.   -45.   -15.     1.     2.     4.   -84.  ]\n",
      " [ 54.     0.   -51.   -48.   -16.     1.     3.     3.   -91.08]\n",
      " [  2.     8.   -53.   -50.   -20.     2.     2.     4.   -97.06]\n",
      " [  2.     6.   -54.   -53.   -21.     1.     3.     3.   -90.04]\n",
      " [  2.     4.   -57.   -56.   -23.     3.     3.     4.     0.  ]]\n",
      "========= Step:  23 =========\n",
      "State:          [  6 -54 -53 -21   1   3   3]\n",
      "Action:         [0 0 0 2]\n",
      "Reward:         -90.04\n",
      "Next state:     [  4 -57 -56 -23   3   3   4]\n",
      "Episode reward: -144.76\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -0.05\n",
      "Next state:     [5 0 0 0 0 0 0]\n",
      "Episode reward: -0.05\n",
      "========= Step:   1 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         33.96\n",
      "Next state:     [ 4 -2 -3 -3  2  4  3]\n",
      "Episode reward: 30.51\n",
      "========= Step:   2 =========\n",
      "State:          [ 4 -2 -3 -3  2  4  3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         33.97\n",
      "Next state:     [ 3 -6 -6 -6  4  4  3]\n",
      "Episode reward: 58.03\n",
      "========= Step:   3 =========\n",
      "State:          [ 3 -6 -6 -6  4  4  3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         15.98\n",
      "Next state:     [ 2 -9 -8 -9  3  3  3]\n",
      "Episode reward: 69.68\n",
      "========= Step:   4 =========\n",
      "State:          [ 2 -9 -8 -9  3  3  3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         11.99\n",
      "Next state:     [  1 -13 -10 -12   4   3   3]\n",
      "Episode reward: 77.55\n",
      "========= Step:   5 =========\n",
      "State:          [  1 -13 -10 -12   4   3   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -5.0\n",
      "Next state:     [  0 -16 -12 -14   3   3   2]\n",
      "Episode reward: 74.59\n",
      "========= Step:   6 =========\n",
      "State:          [  0 -16 -12 -14   3   3   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -18.08\n",
      "Next state:     [  8 -19 -15 -16   3   3   2]\n",
      "Episode reward: 64.98\n",
      "========= Step:   7 =========\n",
      "State:          [  8 -19 -15 -16   3   3   2]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -24.07\n",
      "Next state:     [  7 -22 -16 -18   3   2   2]\n",
      "Episode reward: 53.47\n",
      "========= Step:   8 =========\n",
      "State:          [  7 -22 -16 -18   3   2   2]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -26.06\n",
      "Next state:     [  6 -26 -18 -19   4   3   1]\n",
      "Episode reward: 42.25\n",
      "========= Step:   9 =========\n",
      "State:          [  6 -26 -18 -19   4   3   1]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -33.05\n",
      "Next state:     [  5 -30 -19 -21   4   2   2]\n",
      "Episode reward: 29.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  10 =========\n",
      "State:          [  5 -30 -19 -21   4   2   2]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -44.04\n",
      "Next state:     [  4 -34 -19 -23   4   1   2]\n",
      "Episode reward: 14.09\n",
      "========= Step:  11 =========\n",
      "State:          [  4 -34 -19 -23   4   1   2]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -58.03\n",
      "Next state:     [  3 -37 -19 -24   3   1   1]\n",
      "Episode reward: -4.12\n",
      "========= Step:  12 =========\n",
      "State:          [  3 -37 -19 -24   3   1   1]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -58.02\n",
      "Next state:     [  2 -40 -20 -25   3   2   1]\n",
      "Episode reward: -20.5\n",
      "========= Step:  13 =========\n",
      "State:          [  2 -40 -20 -25   3   2   1]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -63.01\n",
      "Next state:     [  1 -43 -20 -27   3   1   2]\n",
      "Episode reward: -36.52\n",
      "========= Step:  14 =========\n",
      "State:          [  1 -43 -20 -27   3   1   2]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -76.0\n",
      "Next state:     [  0 -44 -21 -28   1   2   1]\n",
      "Episode reward: -53.91\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -44 -21 -28   1   2   1]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -89.08\n",
      "Next state:     [  8 -45 -22 -29   1   1   1]\n",
      "Episode reward: -72.25\n",
      "========= Step:  16 =========\n",
      "State:          [  8 -45 -22 -29   1   1   1]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -70.07\n",
      "Next state:     [  7 -47 -23 -32   2   2   3]\n",
      "Episode reward: -85.23\n",
      "========= Step:  17 =========\n",
      "State:          [  7 -47 -23 -32   2   2   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -80.06\n",
      "Next state:     [  6 -49 -23 -35   2   1   3]\n",
      "Episode reward: -98.58\n",
      "========= Step:  18 =========\n",
      "State:          [  6 -49 -23 -35   2   1   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -85.05\n",
      "Next state:     [  5 -49 -24 -39   0   2   4]\n",
      "Episode reward: -111.35\n",
      "========= Step:  19 =========\n",
      "State:          [  5 -49 -24 -39   0   2   4]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -86.04\n",
      "Next state:     [  4 -50 -25 -43   1   2   4]\n",
      "Episode reward: -122.97\n",
      "========= Step:  20 =========\n",
      "State:          [  4 -50 -25 -43   1   2   4]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -88.03\n",
      "Next state:     [  3 -52 -27 -46   2   3   3]\n",
      "Episode reward: -133.67\n",
      "========= Step:  21 =========\n",
      "State:          [  3 -52 -27 -46   2   3   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -91.02\n",
      "Next state:     [  2 -54 -29 -50   2   3   4]\n",
      "Episode reward: -143.63\n",
      "========= Step:  22 =========\n",
      "State:          [  2 -54 -29 -50   2   3   4]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -99.01\n",
      "Next state:     [  1 -56 -32 -53   2   4   3]\n",
      "Episode reward: -153.38\n",
      "Update: [[  0.00000000e+00   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -4.99999989e-02]\n",
      " [  3.00000000e+00   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.39600000e+01]\n",
      " [  3.00000000e+00   4.00000000e+00  -2.00000000e+00  -3.00000000e+00\n",
      "   -3.00000000e+00   2.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    3.39700000e+01]\n",
      " [  3.00000000e+00   3.00000000e+00  -6.00000000e+00  -6.00000000e+00\n",
      "   -6.00000000e+00   4.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    1.59800000e+01]\n",
      " [  3.00000000e+00   2.00000000e+00  -9.00000000e+00  -8.00000000e+00\n",
      "   -9.00000000e+00   3.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "    1.19900000e+01]\n",
      " [  3.00000000e+00   1.00000000e+00  -1.30000000e+01  -1.00000000e+01\n",
      "   -1.20000000e+01   4.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "   -5.00000000e+00]\n",
      " [  5.40000000e+01   0.00000000e+00  -1.60000000e+01  -1.20000000e+01\n",
      "   -1.40000000e+01   3.00000000e+00   3.00000000e+00   2.00000000e+00\n",
      "   -1.80800000e+01]\n",
      " [  3.00000000e+00   8.00000000e+00  -1.90000000e+01  -1.50000000e+01\n",
      "   -1.60000000e+01   3.00000000e+00   3.00000000e+00   2.00000000e+00\n",
      "   -2.40700000e+01]\n",
      " [  3.00000000e+00   7.00000000e+00  -2.20000000e+01  -1.60000000e+01\n",
      "   -1.80000000e+01   3.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "   -2.60600000e+01]\n",
      " [  3.00000000e+00   6.00000000e+00  -2.60000000e+01  -1.80000000e+01\n",
      "   -1.90000000e+01   4.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   -3.30500000e+01]\n",
      " [  3.00000000e+00   5.00000000e+00  -3.00000000e+01  -1.90000000e+01\n",
      "   -2.10000000e+01   4.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "   -4.40400000e+01]\n",
      " [  3.00000000e+00   4.00000000e+00  -3.40000000e+01  -1.90000000e+01\n",
      "   -2.30000000e+01   4.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -5.80300000e+01]\n",
      " [  3.00000000e+00   3.00000000e+00  -3.70000000e+01  -1.90000000e+01\n",
      "   -2.40000000e+01   3.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -5.80200000e+01]\n",
      " [  3.00000000e+00   2.00000000e+00  -4.00000000e+01  -2.00000000e+01\n",
      "   -2.50000000e+01   3.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "   -6.30100000e+01]\n",
      " [  3.00000000e+00   1.00000000e+00  -4.30000000e+01  -2.00000000e+01\n",
      "   -2.70000000e+01   3.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -7.60000000e+01]\n",
      " [  5.40000000e+01   0.00000000e+00  -4.40000000e+01  -2.10000000e+01\n",
      "   -2.80000000e+01   1.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "   -8.90800000e+01]\n",
      " [  3.00000000e+00   8.00000000e+00  -4.50000000e+01  -2.20000000e+01\n",
      "   -2.90000000e+01   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -7.00700000e+01]\n",
      " [  3.00000000e+00   7.00000000e+00  -4.70000000e+01  -2.30000000e+01\n",
      "   -3.20000000e+01   2.00000000e+00   2.00000000e+00   3.00000000e+00\n",
      "   -8.00600000e+01]\n",
      " [  3.00000000e+00   6.00000000e+00  -4.90000000e+01  -2.30000000e+01\n",
      "   -3.50000000e+01   2.00000000e+00   1.00000000e+00   3.00000000e+00\n",
      "   -8.50500000e+01]\n",
      " [  3.00000000e+00   5.00000000e+00  -4.90000000e+01  -2.40000000e+01\n",
      "   -3.90000000e+01   0.00000000e+00   2.00000000e+00   4.00000000e+00\n",
      "   -8.60400000e+01]\n",
      " [  3.00000000e+00   4.00000000e+00  -5.00000000e+01  -2.50000000e+01\n",
      "   -4.30000000e+01   1.00000000e+00   2.00000000e+00   4.00000000e+00\n",
      "   -8.80300000e+01]\n",
      " [  3.00000000e+00   3.00000000e+00  -5.20000000e+01  -2.70000000e+01\n",
      "   -4.60000000e+01   2.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "   -9.10200000e+01]\n",
      " [  3.00000000e+00   2.00000000e+00  -5.40000000e+01  -2.90000000e+01\n",
      "   -5.00000000e+01   2.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "   -9.90100000e+01]\n",
      " [  3.00000000e+00   1.00000000e+00  -5.60000000e+01  -3.20000000e+01\n",
      "   -5.30000000e+01   2.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "   -1.03000000e+02]\n",
      " [  0.00000000e+00   0.00000000e+00  -5.90000000e+01  -3.40000000e+01\n",
      "   -5.70000000e+01   3.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "    0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py:61: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prob = prob/sum_exp\n",
      "C:\\Users\\henri\\Documents\\Uni\\Uni 2018 Printemps\\INF581 Advanced Topics in Artificial Intelligence\\project\\INF581\\code\\reinforce.py:65: RuntimeWarning: invalid value encountered in less\n",
      "  action = random.choice(len(prob), size = None, p = prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  23 =========\n",
      "State:          [  1 -56 -32 -53   2   4   3]\n",
      "Action:         [0 0 1 0]\n",
      "Reward:         -103.0\n",
      "Next state:     [  0 -59 -34 -57   3   3   4]\n",
      "Episode reward: -162.51\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -0.05\n",
      "Next state:     [5 0 0 0 0 0 0]\n",
      "Episode reward: -0.05\n",
      "========= Step:   1 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         35.95\n",
      "Next state:     [ 5 -3 -3 -3  3  3  3]\n",
      "Episode reward: 32.31\n",
      "========= Step:   2 =========\n",
      "State:          [ 5 -3 -3 -3  3  3  3]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         25.97\n",
      "Next state:     [ 3 -7 -5 -5  4  3  3]\n",
      "Episode reward: 53.34\n",
      "========= Step:   3 =========\n",
      "State:          [ 3 -7 -5 -5  4  3  3]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         21.99\n",
      "Next state:     [  1 -11  -8  -7   4   4   3]\n",
      "Episode reward: 69.37\n",
      "========= Step:   4 =========\n",
      "State:          [  1 -11  -8  -7   4   4   3]\n",
      "Action:         [8 1 0 0]\n",
      "Reward:         4.92\n",
      "Next state:     [  8 -14 -12  -9   4   4   2]\n",
      "Episode reward: 72.6\n",
      "========= Step:   5 =========\n",
      "State:          [  8 -14 -12  -9   4   4   2]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -4.06\n",
      "Next state:     [  6 -17 -15 -10   3   4   2]\n",
      "Episode reward: 70.2\n",
      "========= Step:   6 =========\n",
      "State:          [  6 -17 -15 -10   3   4   2]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -15.04\n",
      "Next state:     [  4 -21 -17 -10   4   3   1]\n",
      "Episode reward: 62.21\n",
      "========= Step:   7 =========\n",
      "State:          [  4 -21 -17 -10   4   3   1]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -17.02\n",
      "Next state:     [  2 -25 -19 -11   4   3   2]\n",
      "Episode reward: 54.07\n",
      "========= Step:   8 =========\n",
      "State:          [  2 -25 -19 -11   4   3   2]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -28.0\n",
      "Next state:     [  0 -29 -21 -11   4   3   1]\n",
      "Episode reward: 42.02\n",
      "========= Step:   9 =========\n",
      "State:          [  0 -29 -21 -11   4   3   1]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -37.04\n",
      "Next state:     [  4 -32 -24 -12   3   3   1]\n",
      "Episode reward: 27.67\n",
      "========= Step:  10 =========\n",
      "State:          [  4 -32 -24 -12   3   3   1]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -53.02\n",
      "Next state:     [  2 -35 -24 -12   3   1   1]\n",
      "Episode reward: 9.18\n",
      "========= Step:  11 =========\n",
      "State:          [  2 -35 -24 -12   3   1   1]\n",
      "Action:         [0 0 1 1]\n",
      "Reward:         -56.0\n",
      "Next state:     [  0 -37 -24 -13   2   1   2]\n",
      "Episode reward: -8.39\n",
      "========= Step:  12 =========\n",
      "State:          [  0 -37 -24 -13   2   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -50.0\n",
      "Next state:     [  0 -40 -25 -15   3   1   2]\n",
      "Episode reward: -22.52\n",
      "========= Step:  13 =========\n",
      "State:          [  0 -40 -25 -15   3   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -56.0\n",
      "Next state:     [  0 -43 -27 -16   3   2   1]\n",
      "Episode reward: -36.75\n",
      "========= Step:  14 =========\n",
      "State:          [  0 -43 -27 -16   3   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -66.0\n",
      "Next state:     [  0 -44 -29 -18   1   2   2]\n",
      "Episode reward: -51.85\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -44 -29 -18   1   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -71.0\n",
      "Next state:     [  0 -45 -31 -20   1   2   2]\n",
      "Episode reward: -66.47\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -45 -31 -20   1   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -76.0\n",
      "Next state:     [  0 -46 -32 -23   1   1   3]\n",
      "Episode reward: -80.55\n",
      "========= Step:  17 =========\n",
      "State:          [  0 -46 -32 -23   1   1   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -85.0\n",
      "Next state:     [  0 -47 -33 -25   1   1   2]\n",
      "Episode reward: -94.73\n",
      "========= Step:  18 =========\n",
      "State:          [  0 -47 -33 -25   1   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -85.0\n",
      "Next state:     [  0 -47 -35 -28   0   2   3]\n",
      "Episode reward: -107.48\n",
      "========= Step:  19 =========\n",
      "State:          [  0 -47 -35 -28   0   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -82.0\n",
      "Next state:     [  0 -49 -37 -31   2   2   3]\n",
      "Episode reward: -118.56\n",
      "========= Step:  20 =========\n",
      "State:          [  0 -49 -37 -31   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -85.0\n",
      "Next state:     [  0 -51 -39 -35   2   2   4]\n",
      "Episode reward: -128.9\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -51 -39 -35   2   2   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -97.0\n",
      "Next state:     [  0 -52 -42 -38   1   3   3]\n",
      "Episode reward: -139.51\n",
      "========= Step:  22 =========\n",
      "State:          [  0 -52 -42 -38   1   3   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -96.0\n",
      "Next state:     [  0 -54 -45 -42   2   3   4]\n",
      "Episode reward: -148.96\n",
      "Update: [[  0.00000000e+00   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -4.99999989e-02]\n",
      " [  0.00000000e+00   5.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.59500000e+01]\n",
      " [  4.00000000e+00   5.00000000e+00  -3.00000000e+00  -3.00000000e+00\n",
      "   -3.00000000e+00   3.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "    2.59700000e+01]\n",
      " [  4.00000000e+00   3.00000000e+00  -7.00000000e+00  -5.00000000e+00\n",
      "   -5.00000000e+00   4.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "    2.19900000e+01]\n",
      " [  6.30000000e+01   1.00000000e+00  -1.10000000e+01  -8.00000000e+00\n",
      "   -7.00000000e+00   4.00000000e+00   4.00000000e+00   3.00000000e+00\n",
      "    4.92000000e+00]\n",
      " [  4.00000000e+00   8.00000000e+00  -1.40000000e+01  -1.20000000e+01\n",
      "   -9.00000000e+00   4.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   -4.06000000e+00]\n",
      " [  4.00000000e+00   6.00000000e+00  -1.70000000e+01  -1.50000000e+01\n",
      "   -1.00000000e+01   3.00000000e+00   4.00000000e+00   2.00000000e+00\n",
      "   -1.50400000e+01]\n",
      " [  4.00000000e+00   4.00000000e+00  -2.10000000e+01  -1.70000000e+01\n",
      "   -1.00000000e+01   4.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   -1.70200000e+01]\n",
      " [  4.00000000e+00   2.00000000e+00  -2.50000000e+01  -1.90000000e+01\n",
      "   -1.10000000e+01   4.00000000e+00   3.00000000e+00   2.00000000e+00\n",
      "   -2.80000000e+01]\n",
      " [  2.70000000e+01   0.00000000e+00  -2.90000000e+01  -2.10000000e+01\n",
      "   -1.10000000e+01   4.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   -3.70400000e+01]\n",
      " [  4.00000000e+00   4.00000000e+00  -3.20000000e+01  -2.40000000e+01\n",
      "   -1.20000000e+01   3.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   -5.30200000e+01]\n",
      " [  4.00000000e+00   2.00000000e+00  -3.50000000e+01  -2.40000000e+01\n",
      "   -1.20000000e+01   3.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   -5.60000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -3.70000000e+01  -2.40000000e+01\n",
      "   -1.30000000e+01   2.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -5.00000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.00000000e+01  -2.50000000e+01\n",
      "   -1.50000000e+01   3.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -5.60000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.30000000e+01  -2.70000000e+01\n",
      "   -1.60000000e+01   3.00000000e+00   2.00000000e+00   1.00000000e+00\n",
      "   -6.60000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.40000000e+01  -2.90000000e+01\n",
      "   -1.80000000e+01   1.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "   -7.10000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.50000000e+01  -3.10000000e+01\n",
      "   -2.00000000e+01   1.00000000e+00   2.00000000e+00   2.00000000e+00\n",
      "   -7.60000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.60000000e+01  -3.20000000e+01\n",
      "   -2.30000000e+01   1.00000000e+00   1.00000000e+00   3.00000000e+00\n",
      "   -8.50000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.70000000e+01  -3.30000000e+01\n",
      "   -2.50000000e+01   1.00000000e+00   1.00000000e+00   2.00000000e+00\n",
      "   -8.50000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.70000000e+01  -3.50000000e+01\n",
      "   -2.80000000e+01   0.00000000e+00   2.00000000e+00   3.00000000e+00\n",
      "   -8.20000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -4.90000000e+01  -3.70000000e+01\n",
      "   -3.10000000e+01   2.00000000e+00   2.00000000e+00   3.00000000e+00\n",
      "   -8.50000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -5.10000000e+01  -3.90000000e+01\n",
      "   -3.50000000e+01   2.00000000e+00   2.00000000e+00   4.00000000e+00\n",
      "   -9.70000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -5.20000000e+01  -4.20000000e+01\n",
      "   -3.80000000e+01   1.00000000e+00   3.00000000e+00   3.00000000e+00\n",
      "   -9.60000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -5.40000000e+01  -4.50000000e+01\n",
      "   -4.20000000e+01   2.00000000e+00   3.00000000e+00   4.00000000e+00\n",
      "   -9.70000000e+01]\n",
      " [  0.00000000e+00   0.00000000e+00  -5.70000000e+01  -4.90000000e+01\n",
      "   -4.60000000e+01   3.00000000e+00   4.00000000e+00   4.00000000e+00\n",
      "    0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  23 =========\n",
      "State:          [  0 -54 -45 -42   2   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -97.0\n",
      "Next state:     [  0 -57 -49 -46   3   4   4]\n",
      "Episode reward: -157.56\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [8 0 0 2]\n",
      "Reward:         -12.31\n",
      "Next state:     [11  0  0  2  0  0  0]\n",
      "Episode reward: -12.31\n",
      "========= Step:   1 =========\n",
      "State:          [11  0  0  2  0  0  0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         41.89\n",
      "Next state:     [11 -3 -4 -1  3  4  3]\n",
      "Episode reward: 25.39\n",
      "========= Step:   2 =========\n",
      "State:          [11 -3 -4 -1  3  4  3]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         31.92\n",
      "Next state:     [ 8 -7 -6 -3  4  3  4]\n",
      "Episode reward: 51.25\n",
      "========= Step:   3 =========\n",
      "State:          [ 8 -7 -6 -3  4  3  4]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         23.95\n",
      "Next state:     [  5 -11  -9  -4   4   4   3]\n",
      "Episode reward: 68.71\n",
      "========= Step:   4 =========\n",
      "State:          [  5 -11  -9  -4   4   4   3]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         11.98\n",
      "Next state:     [  2 -15 -11  -5   4   3   3]\n",
      "Episode reward: 76.57\n",
      "========= Step:   5 =========\n",
      "State:          [  2 -15 -11  -5   4   3   3]\n",
      "Action:         [0 2 0 0]\n",
      "Reward:         5.0\n",
      "Next state:     [  0 -17 -14  -7   4   3   2]\n",
      "Episode reward: 79.52\n",
      "========= Step:   6 =========\n",
      "State:          [  0 -17 -14  -7   4   3   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -10.08\n",
      "Next state:     [  8 -20 -18  -9   3   4   2]\n",
      "Episode reward: 74.16\n",
      "========= Step:   7 =========\n",
      "State:          [  8 -20 -18  -9   3   4   2]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -23.05\n",
      "Next state:     [  5 -23 -19  -9   3   2   2]\n",
      "Episode reward: 63.14\n",
      "========= Step:   8 =========\n",
      "State:          [  5 -23 -19  -9   3   2   2]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -19.02\n",
      "Next state:     [  2 -27 -21  -9   4   3   2]\n",
      "Episode reward: 54.95\n",
      "========= Step:   9 =========\n",
      "State:          [  2 -27 -21  -9   4   3   2]\n",
      "Action:         [0 1 0 1]\n",
      "Reward:         -33.0\n",
      "Next state:     [  0 -29 -23 -10   3   2   2]\n",
      "Episode reward: 42.16\n",
      "========= Step:  10 =========\n",
      "State:          [  0 -29 -23 -10   3   2   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -42.08\n",
      "Next state:     [  8 -33 -24 -12   4   1   2]\n",
      "Episode reward: 27.49\n",
      "========= Step:  11 =========\n",
      "State:          [  8 -33 -24 -12   4   1   2]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -49.05\n",
      "Next state:     [  5 -36 -25 -11   3   2   1]\n",
      "Episode reward: 12.1\n",
      "========= Step:  12 =========\n",
      "State:          [  5 -36 -25 -11   3   2   1]\n",
      "Action:         [0 0 1 2]\n",
      "Reward:         -48.02\n",
      "Next state:     [  2 -39 -26 -11   3   2   2]\n",
      "Episode reward: -1.46\n",
      "========= Step:  13 =========\n",
      "State:          [  2 -39 -26 -11   3   2   2]\n",
      "Action:         [0 1 0 1]\n",
      "Reward:         -56.0\n",
      "Next state:     [  0 -40 -28 -12   2   2   2]\n",
      "Episode reward: -15.7\n",
      "========= Step:  14 =========\n",
      "State:          [  0 -40 -28 -12   2   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -64.0\n",
      "Next state:     [  0 -41 -30 -13   1   2   1]\n",
      "Episode reward: -30.34\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -41 -30 -13   1   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -64.0\n",
      "Next state:     [  0 -43 -31 -15   2   1   2]\n",
      "Episode reward: -43.52\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -43 -31 -15   2   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -69.0\n",
      "Next state:     [  0 -44 -33 -17   1   2   2]\n",
      "Episode reward: -56.3\n",
      "========= Step:  17 =========\n",
      "State:          [  0 -44 -33 -17   1   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -70.0\n",
      "Next state:     [  0 -46 -35 -19   2   2   2]\n",
      "Episode reward: -67.98\n",
      "========= Step:  18 =========\n",
      "State:          [  0 -46 -35 -19   2   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -76.0\n",
      "Next state:     [  0 -46 -37 -23   0   2   4]\n",
      "Episode reward: -79.38\n",
      "========= Step:  19 =========\n",
      "State:          [  0 -46 -37 -23   0   2   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -82.0\n",
      "Next state:     [  0 -47 -39 -26   1   2   3]\n",
      "Episode reward: -90.46\n",
      "========= Step:  20 =========\n",
      "State:          [  0 -47 -39 -26   1   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -80.0\n",
      "Next state:     [  0 -48 -42 -30   1   3   4]\n",
      "Episode reward: -100.19\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -48 -42 -30   1   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -84.0\n",
      "Next state:     [  0 -50 -45 -34   2   3   4]\n",
      "Episode reward: -109.38\n",
      "========= Step:  22 =========\n",
      "State:          [  0 -50 -45 -34   2   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -93.0\n",
      "Next state:     [  0 -51 -49 -38   1   4   4]\n",
      "Episode reward: -118.54\n",
      "Update: [[ 56.     5.     0.     0.     0.     0.     0.     0.   -12.31]\n",
      " [  0.    11.     0.     0.     2.     0.     0.     0.    41.89]\n",
      " [  5.    11.    -3.    -4.    -1.     3.     4.     3.    31.92]\n",
      " [  5.     8.    -7.    -6.    -3.     4.     3.     4.    23.95]\n",
      " [  5.     5.   -11.    -9.    -4.     4.     4.     3.    11.98]\n",
      " [ 18.     2.   -15.   -11.    -5.     4.     3.     3.     5.  ]\n",
      " [ 54.     0.   -17.   -14.    -7.     4.     3.     2.   -10.08]\n",
      " [  5.     8.   -20.   -18.    -9.     3.     4.     2.   -23.05]\n",
      " [  5.     5.   -23.   -19.    -9.     3.     2.     2.   -19.02]\n",
      " [ 10.     2.   -27.   -21.    -9.     4.     3.     2.   -33.  ]\n",
      " [ 54.     0.   -29.   -23.   -10.     3.     2.     2.   -42.08]\n",
      " [  5.     8.   -33.   -24.   -12.     4.     1.     2.   -49.05]\n",
      " [  5.     5.   -36.   -25.   -11.     3.     2.     1.   -48.02]\n",
      " [ 10.     2.   -39.   -26.   -11.     3.     2.     2.   -56.  ]\n",
      " [  0.     0.   -40.   -28.   -12.     2.     2.     2.   -64.  ]\n",
      " [  0.     0.   -41.   -30.   -13.     1.     2.     1.   -64.  ]\n",
      " [  0.     0.   -43.   -31.   -15.     2.     1.     2.   -69.  ]\n",
      " [  0.     0.   -44.   -33.   -17.     1.     2.     2.   -70.  ]\n",
      " [  0.     0.   -46.   -35.   -19.     2.     2.     2.   -76.  ]\n",
      " [  0.     0.   -46.   -37.   -23.     0.     2.     4.   -82.  ]\n",
      " [  0.     0.   -47.   -39.   -26.     1.     2.     3.   -80.  ]\n",
      " [  0.     0.   -48.   -42.   -30.     1.     3.     4.   -84.  ]\n",
      " [  0.     0.   -50.   -45.   -34.     2.     3.     4.   -93.  ]\n",
      " [  0.     0.   -51.   -49.   -38.     1.     4.     4.   -94.  ]\n",
      " [  0.     0.   -54.   -53.   -42.     3.     4.     4.     0.  ]]\n",
      "========= Step:  23 =========\n",
      "State:          [  0 -51 -49 -38   1   4   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -94.0\n",
      "Next state:     [  0 -54 -53 -42   3   4   4]\n",
      "Episode reward: -126.87\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -4.14\n",
      "Next state:     [4 0 0 1 0 0 0]\n",
      "Episode reward: -4.14\n",
      "========= Step:   1 =========\n",
      "State:          [4 0 0 1 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         36.96\n",
      "Next state:     [ 4 -2 -4 -2  2  4  3]\n",
      "Episode reward: 29.12\n",
      "========= Step:   2 =========\n",
      "State:          [ 4 -2 -4 -2  2  4  3]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         34.98\n",
      "Next state:     [ 2 -6 -6 -5  4  4  3]\n",
      "Episode reward: 57.46\n",
      "========= Step:   3 =========\n",
      "State:          [ 2 -6 -6 -5  4  4  3]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         26.0\n",
      "Next state:     [  0 -10  -7  -9   4   3   4]\n",
      "Episode reward: 76.41\n",
      "========= Step:   4 =========\n",
      "State:          [  0 -10  -7  -9   4   3   4]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         13.96\n",
      "Next state:     [  4 -14 -11 -12   4   4   3]\n",
      "Episode reward: 85.57\n",
      "========= Step:   5 =========\n",
      "State:          [  4 -14 -11 -12   4   4   3]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         1.98\n",
      "Next state:     [  2 -18 -13 -14   4   4   2]\n",
      "Episode reward: 86.74\n",
      "========= Step:   6 =========\n",
      "State:          [  2 -18 -13 -14   4   4   2]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -14.0\n",
      "Next state:     [  0 -22 -14 -15   4   3   1]\n",
      "Episode reward: 79.3\n",
      "========= Step:   7 =========\n",
      "State:          [  0 -22 -14 -15   4   3   1]\n",
      "Action:         [4 0 0 0]\n",
      "Reward:         -27.04\n",
      "Next state:     [  4 -25 -16 -17   3   2   2]\n",
      "Episode reward: 66.37\n",
      "========= Step:   8 =========\n",
      "State:          [  4 -25 -16 -17   3   2   2]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -31.02\n",
      "Next state:     [  2 -29 -16 -18   4   2   1]\n",
      "Episode reward: 53.01\n",
      "========= Step:   9 =========\n",
      "State:          [  2 -29 -16 -18   4   2   1]\n",
      "Action:         [0 0 2 0]\n",
      "Reward:         -32.0\n",
      "Next state:     [  0 -32 -17 -20   3   3   2]\n",
      "Episode reward: 40.62\n",
      "========= Step:  10 =========\n",
      "State:          [  0 -32 -17 -20   3   3   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -49.0\n",
      "Next state:     [  0 -35 -18 -21   3   1   1]\n",
      "Episode reward: 23.53\n",
      "========= Step:  11 =========\n",
      "State:          [  0 -35 -18 -21   3   1   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -50.0\n",
      "Next state:     [  0 -38 -20 -22   3   2   1]\n",
      "Episode reward: 7.84\n",
      "========= Step:  12 =========\n",
      "State:          [  0 -38 -20 -22   3   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -60.0\n",
      "Next state:     [  0 -40 -21 -24   2   1   2]\n",
      "Episode reward: -9.11\n",
      "========= Step:  13 =========\n",
      "State:          [  0 -40 -21 -24   2   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -61.0\n",
      "Next state:     [  0 -43 -23 -25   3   2   1]\n",
      "Episode reward: -24.61\n",
      "========= Step:  14 =========\n",
      "State:          [  0 -43 -23 -25   3   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -71.0\n",
      "Next state:     [  0 -45 -25 -26   2   2   1]\n",
      "Episode reward: -40.85\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -45 -25 -26   2   2   1]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -76.0\n",
      "Next state:     [  0 -46 -27 -28   1   2   2]\n",
      "Episode reward: -56.5\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -46 -27 -28   1   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -77.0\n",
      "Next state:     [  0 -48 -28 -31   2   1   3]\n",
      "Episode reward: -70.77\n",
      "========= Step:  17 =========\n",
      "State:          [  0 -48 -28 -31   2   1   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -79.0\n",
      "Next state:     [  0 -50 -30 -34   2   2   3]\n",
      "Episode reward: -83.94\n",
      "========= Step:  18 =========\n",
      "State:          [  0 -50 -30 -34   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -94.0\n",
      "Next state:     [  0 -50 -32 -37   0   2   3]\n",
      "Episode reward: -98.05\n",
      "========= Step:  19 =========\n",
      "State:          [  0 -50 -32 -37   0   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -87.0\n",
      "Next state:     [  0 -51 -35 -41   1   3   4]\n",
      "Episode reward: -109.81\n",
      "========= Step:  20 =========\n",
      "State:          [  0 -51 -35 -41   1   3   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -99.0\n",
      "Next state:     [  0 -53 -37 -44   2   2   3]\n",
      "Episode reward: -121.84\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -53 -37 -44   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -106.0\n",
      "Next state:     [  0 -54 -40 -47   1   3   3]\n",
      "Episode reward: -133.44\n",
      "========= Step:  22 =========\n",
      "State:          [  0 -54 -40 -47   1   3   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -105.0\n",
      "Next state:     [  0 -55 -44 -51   1   4   4]\n",
      "Episode reward: -143.78\n",
      "Update: [[   1.      5.      0.      0.      0.      0.      0.      0.     -4.14]\n",
      " [   0.      4.      0.      0.      1.      0.      0.      0.     36.96]\n",
      " [   6.      4.     -2.     -4.     -2.      2.      4.      3.     34.98]\n",
      " [   6.      2.     -6.     -6.     -5.      4.      4.      3.     26.  ]\n",
      " [  27.      0.    -10.     -7.     -9.      4.      3.      4.     13.96]\n",
      " [   6.      4.    -14.    -11.    -12.      4.      4.      3.      1.98]\n",
      " [   6.      2.    -18.    -13.    -14.      4.      4.      2.    -14.  ]\n",
      " [  27.      0.    -22.    -14.    -15.      4.      3.      1.    -27.04]\n",
      " [   6.      4.    -25.    -16.    -17.      3.      2.      2.    -31.02]\n",
      " [   6.      2.    -29.    -16.    -18.      4.      2.      1.    -32.  ]\n",
      " [   0.      0.    -32.    -17.    -20.      3.      3.      2.    -49.  ]\n",
      " [   0.      0.    -35.    -18.    -21.      3.      1.      1.    -50.  ]\n",
      " [   0.      0.    -38.    -20.    -22.      3.      2.      1.    -60.  ]\n",
      " [   0.      0.    -40.    -21.    -24.      2.      1.      2.    -61.  ]\n",
      " [   0.      0.    -43.    -23.    -25.      3.      2.      1.    -71.  ]\n",
      " [   0.      0.    -45.    -25.    -26.      2.      2.      1.    -76.  ]\n",
      " [   0.      0.    -46.    -27.    -28.      1.      2.      2.    -77.  ]\n",
      " [   0.      0.    -48.    -28.    -31.      2.      1.      3.    -79.  ]\n",
      " [   0.      0.    -50.    -30.    -34.      2.      2.      3.    -94.  ]\n",
      " [   0.      0.    -50.    -32.    -37.      0.      2.      3.    -87.  ]\n",
      " [   0.      0.    -51.    -35.    -41.      1.      3.      4.    -99.  ]\n",
      " [   0.      0.    -53.    -37.    -44.      2.      2.      3.   -106.  ]\n",
      " [   0.      0.    -54.    -40.    -47.      1.      3.      3.   -105.  ]\n",
      " [   0.      0.    -55.    -44.    -51.      1.      4.      4.   -110.  ]\n",
      " [   0.      0.    -58.    -48.    -54.      3.      4.      3.      0.  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Step:  23 =========\n",
      "State:          [  0 -55 -44 -51   1   4   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -110.0\n",
      "Next state:     [  0 -58 -48 -54   3   4   3]\n",
      "Episode reward: -153.53\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -4.14\n",
      "Next state:     [4 0 0 1 0 0 0]\n",
      "Episode reward: -4.14\n",
      "========= Step:   1 =========\n",
      "State:          [4 0 0 1 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         40.96\n",
      "Next state:     [ 4 -3 -3 -3  3  3  4]\n",
      "Episode reward: 32.72\n",
      "========= Step:   2 =========\n",
      "State:          [ 4 -3 -3 -3  3  3  4]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         27.97\n",
      "Next state:     [ 3 -6 -6 -6  3  3  4]\n",
      "Episode reward: 55.38\n",
      "========= Step:   3 =========\n",
      "State:          [ 3 -6 -6 -6  3  3  4]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         19.95\n",
      "Next state:     [  5 -10 -10  -7   4   4   3]\n",
      "Episode reward: 69.92\n",
      "========= Step:   4 =========\n",
      "State:          [  5 -10 -10  -7   4   4   3]\n",
      "Action:         [4 0 1 0]\n",
      "Reward:         6.92\n",
      "Next state:     [  8 -14 -12 -10   4   3   3]\n",
      "Episode reward: 74.46\n",
      "========= Step:   5 =========\n",
      "State:          [  8 -14 -12 -10   4   3   3]\n",
      "Action:         [0 0 2 1]\n",
      "Reward:         -4.05\n",
      "Next state:     [  5 -18 -13 -11   4   3   2]\n",
      "Episode reward: 72.07\n",
      "========= Step:   6 =========\n",
      "State:          [  5 -18 -13 -11   4   3   2]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -12.07\n",
      "Next state:     [  7 -22 -16 -11   4   3   2]\n",
      "Episode reward: 65.66\n",
      "========= Step:   7 =========\n",
      "State:          [  7 -22 -16 -11   4   3   2]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -23.09\n",
      "Next state:     [  9 -26 -19 -10   4   3   1]\n",
      "Episode reward: 54.61\n",
      "========= Step:   8 =========\n",
      "State:          [  9 -26 -19 -10   4   3   1]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -29.11\n",
      "Next state:     [ 11 -30 -21 -10   4   2   2]\n",
      "Episode reward: 42.08\n",
      "========= Step:   9 =========\n",
      "State:          [ 11 -30 -21 -10   4   2   2]\n",
      "Action:         [4 0 1 0]\n",
      "Reward:         -39.14\n",
      "Next state:     [ 14 -33 -22 -12   3   2   2]\n",
      "Episode reward: 26.92\n",
      "========= Step:  10 =========\n",
      "State:          [ 14 -33 -22 -12   3   2   2]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -53.16\n",
      "Next state:     [ 16 -36 -23 -11   3   1   1]\n",
      "Episode reward: 8.38\n",
      "========= Step:  11 =========\n",
      "State:          [ 16 -36 -23 -11   3   1   1]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -60.18\n",
      "Next state:     [ 18 -38 -24 -10   2   1   1]\n",
      "Episode reward: -10.5\n",
      "========= Step:  12 =========\n",
      "State:          [ 18 -38 -24 -10   2   1   1]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -58.2\n",
      "Next state:     [ 20 -40 -25 -10   2   1   2]\n",
      "Episode reward: -26.94\n",
      "========= Step:  13 =========\n",
      "State:          [ 20 -40 -25 -10   2   1   2]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -58.17\n",
      "Next state:     [ 17 -41 -27  -9   2   2   1]\n",
      "Episode reward: -41.73\n",
      "========= Step:  14 =========\n",
      "State:          [ 17 -41 -27  -9   2   2   1]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -67.19\n",
      "Next state:     [ 19 -42 -29  -8   1   2   1]\n",
      "Episode reward: -57.1\n",
      "========= Step:  15 =========\n",
      "State:          [ 19 -42 -29  -8   1   2   1]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -62.16\n",
      "Next state:     [ 16 -43 -30  -8   2   1   2]\n",
      "Episode reward: -69.89\n",
      "========= Step:  16 =========\n",
      "State:          [ 16 -43 -30  -8   2   1   2]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -63.18\n",
      "Next state:     [ 18 -45 -32  -8   2   2   2]\n",
      "Episode reward: -81.6\n",
      "========= Step:  17 =========\n",
      "State:          [ 18 -45 -32  -8   2   2   2]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -67.2\n",
      "Next state:     [ 20 -47 -33  -9   2   1   3]\n",
      "Episode reward: -92.81\n",
      "========= Step:  18 =========\n",
      "State:          [ 20 -47 -33  -9   2   1   3]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -72.17\n",
      "Next state:     [ 17 -47 -34 -10   1   1   3]\n",
      "Episode reward: -103.64\n",
      "========= Step:  19 =========\n",
      "State:          [ 17 -47 -34 -10   1   1   3]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -65.19\n",
      "Next state:     [ 19 -48 -37 -12   1   3   4]\n",
      "Episode reward: -112.45\n",
      "========= Step:  20 =========\n",
      "State:          [ 19 -48 -37 -12   1   3   4]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -68.16\n",
      "Next state:     [ 16 -49 -39 -14   2   2   4]\n",
      "Episode reward: -120.73\n",
      "========= Step:  21 =========\n",
      "State:          [ 16 -49 -39 -14   2   2   4]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -80.18\n",
      "Next state:     [ 18 -50 -42 -15   1   3   3]\n",
      "Episode reward: -129.51\n",
      "========= Step:  22 =========\n",
      "State:          [ 18 -50 -42 -15   1   3   3]\n",
      "Action:         [4 0 0 2]\n",
      "Reward:         -77.2\n",
      "Next state:     [ 20 -52 -45 -17   2   3   4]\n",
      "Episode reward: -137.11\n",
      "Update: [[  1.     5.     0.     0.     0.     0.     0.     0.    -4.14]\n",
      " [  0.     4.     0.     0.     1.     0.     0.     0.    40.96]\n",
      " [  1.     4.    -3.    -3.    -3.     3.     3.     4.    27.97]\n",
      " [ 29.     3.    -6.    -6.    -6.     3.     3.     4.    19.95]\n",
      " [ 30.     5.   -10.   -10.    -7.     4.     4.     3.     6.92]\n",
      " [  7.     8.   -14.   -12.   -10.     4.     3.     3.    -4.05]\n",
      " [ 29.     5.   -18.   -13.   -11.     4.     3.     2.   -12.07]\n",
      " [ 29.     7.   -22.   -16.   -11.     4.     3.     2.   -23.09]\n",
      " [ 29.     9.   -26.   -19.   -10.     4.     3.     1.   -29.11]\n",
      " [ 30.    11.   -30.   -21.   -10.     4.     2.     2.   -39.14]\n",
      " [ 29.    14.   -33.   -22.   -12.     3.     2.     2.   -53.16]\n",
      " [ 29.    16.   -36.   -23.   -11.     3.     1.     1.   -60.18]\n",
      " [ 29.    18.   -38.   -24.   -10.     2.     1.     1.   -58.2 ]\n",
      " [ 11.    20.   -40.   -25.   -10.     2.     1.     2.   -58.17]\n",
      " [ 29.    17.   -41.   -27.    -9.     2.     2.     1.   -67.19]\n",
      " [ 11.    19.   -42.   -29.    -8.     1.     2.     1.   -62.16]\n",
      " [ 29.    16.   -43.   -30.    -8.     2.     1.     2.   -63.18]\n",
      " [ 29.    18.   -45.   -32.    -8.     2.     2.     2.   -67.2 ]\n",
      " [ 11.    20.   -47.   -33.    -9.     2.     1.     3.   -72.17]\n",
      " [ 29.    17.   -47.   -34.   -10.     1.     1.     3.   -65.19]\n",
      " [ 11.    19.   -48.   -37.   -12.     1.     3.     4.   -68.16]\n",
      " [ 29.    16.   -49.   -39.   -14.     2.     2.     4.   -80.18]\n",
      " [ 29.    18.   -50.   -42.   -15.     1.     3.     3.   -77.2 ]\n",
      " [ 11.    20.   -52.   -45.   -17.     2.     3.     4.   -77.17]\n",
      " [ 29.    17.   -54.   -49.   -18.     3.     4.     3.     0.  ]]\n",
      "========= Step:  23 =========\n",
      "State:          [ 20 -52 -45 -17   2   3   4]\n",
      "Action:         [0 1 0 2]\n",
      "Reward:         -77.17\n",
      "Next state:     [ 17 -54 -49 -18   3   4   3]\n",
      "Episode reward: -143.95\n",
      "========= Step:   0 =========\n",
      "State:          [5 0 0 0 0 0 0]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -4.14\n",
      "Next state:     [4 0 0 1 0 0 0]\n",
      "Episode reward: -4.14\n",
      "========= Step:   1 =========\n",
      "State:          [4 0 0 1 0 0 0]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         40.96\n",
      "Next state:     [ 4 -2 -4 -3  2  4  4]\n",
      "Episode reward: 32.72\n",
      "========= Step:   2 =========\n",
      "State:          [ 4 -2 -4 -3  2  4  4]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         27.97\n",
      "Next state:     [ 3 -6 -7 -5  4  3  3]\n",
      "Episode reward: 55.38\n",
      "========= Step:   3 =========\n",
      "State:          [ 3 -6 -7 -5  4  3  3]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         22.98\n",
      "Next state:     [  2 -10 -11  -7   4   4   3]\n",
      "Episode reward: 72.13\n",
      "========= Step:   4 =========\n",
      "State:          [  2 -10 -11  -7   4   4   3]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         8.99\n",
      "Next state:     [  1 -14 -14  -9   4   3   3]\n",
      "Episode reward: 78.03\n",
      "========= Step:   5 =========\n",
      "State:          [  1 -14 -14  -9   4   3   3]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -4.0\n",
      "Next state:     [  0 -17 -18 -10   3   4   2]\n",
      "Episode reward: 75.67\n",
      "========= Step:   6 =========\n",
      "State:          [  0 -17 -18 -10   3   4   2]\n",
      "Action:         [8 0 0 0]\n",
      "Reward:         -17.08\n",
      "Next state:     [  8 -21 -22 -11   4   4   1]\n",
      "Episode reward: 66.59\n",
      "========= Step:   7 =========\n",
      "State:          [  8 -21 -22 -11   4   4   1]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -25.07\n",
      "Next state:     [  7 -25 -24 -12   4   2   2]\n",
      "Episode reward: 54.6\n",
      "========= Step:   8 =========\n",
      "State:          [  7 -25 -24 -12   4   2   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -36.06\n",
      "Next state:     [  6 -28 -26 -13   3   2   2]\n",
      "Episode reward: 39.08\n",
      "========= Step:   9 =========\n",
      "State:          [  6 -28 -26 -13   3   2   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -42.05\n",
      "Next state:     [  5 -32 -28 -13   4   2   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 22.79\n",
      "========= Step:  10 =========\n",
      "State:          [  5 -32 -28 -13   4   2   1]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -48.04\n",
      "Next state:     [  4 -36 -29 -14   4   1   2]\n",
      "Episode reward: 6.04\n",
      "========= Step:  11 =========\n",
      "State:          [  4 -36 -29 -14   4   1   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -58.03\n",
      "Next state:     [  3 -38 -31 -15   2   2   2]\n",
      "Episode reward: -12.17\n",
      "========= Step:  12 =========\n",
      "State:          [  3 -38 -31 -15   2   2   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -63.02\n",
      "Next state:     [  2 -40 -33 -16   2   2   2]\n",
      "Episode reward: -29.97\n",
      "========= Step:  13 =========\n",
      "State:          [  2 -40 -33 -16   2   2   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -68.01\n",
      "Next state:     [  1 -43 -34 -17   3   1   2]\n",
      "Episode reward: -47.26\n",
      "========= Step:  14 =========\n",
      "State:          [  1 -43 -34 -17   3   1   2]\n",
      "Action:         [0 0 0 1]\n",
      "Reward:         -81.0\n",
      "Next state:     [  0 -44 -35 -18   1   1   2]\n",
      "Episode reward: -65.79\n",
      "========= Step:  15 =========\n",
      "State:          [  0 -44 -35 -18   1   1   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -73.0\n",
      "Next state:     [  0 -46 -37 -20   2   2   2]\n",
      "Episode reward: -80.82\n",
      "========= Step:  16 =========\n",
      "State:          [  0 -46 -37 -20   2   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -79.0\n",
      "Next state:     [  0 -47 -39 -23   1   2   3]\n",
      "Episode reward: -95.46\n",
      "========= Step:  17 =========\n",
      "State:          [  0 -47 -39 -23   1   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -85.0\n",
      "Next state:     [  0 -49 -41 -25   2   2   2]\n",
      "Episode reward: -109.63\n",
      "========= Step:  18 =========\n",
      "State:          [  0 -49 -41 -25   2   2   2]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -87.0\n",
      "Next state:     [  0 -50 -43 -29   1   2   4]\n",
      "Episode reward: -122.69\n",
      "========= Step:  19 =========\n",
      "State:          [  0 -50 -43 -29   1   2   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -94.0\n",
      "Next state:     [  0 -52 -45 -32   2   2   3]\n",
      "Episode reward: -135.39\n",
      "========= Step:  20 =========\n",
      "State:          [  0 -52 -45 -32   2   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -101.0\n",
      "Next state:     [  0 -53 -47 -36   1   2   4]\n",
      "Episode reward: -147.67\n",
      "========= Step:  21 =========\n",
      "State:          [  0 -53 -47 -36   1   2   4]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -112.0\n",
      "Next state:     [  0 -54 -49 -39   1   2   3]\n",
      "Episode reward: -159.93\n",
      "========= Step:  22 =========\n",
      "State:          [  0 -54 -49 -39   1   2   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -106.0\n",
      "Next state:     [  0 -56 -53 -42   2   4   3]\n",
      "Episode reward: -170.36\n",
      "Update: [[   1.      5.      0.      0.      0.      0.      0.      0.     -4.14]\n",
      " [   0.      4.      0.      0.      1.      0.      0.      0.     40.96]\n",
      " [   1.      4.     -2.     -4.     -3.      2.      4.      4.     27.97]\n",
      " [   1.      3.     -6.     -7.     -5.      4.      3.      3.     22.98]\n",
      " [   1.      2.    -10.    -11.     -7.      4.      4.      3.      8.99]\n",
      " [   1.      1.    -14.    -14.     -9.      4.      3.      3.     -4.  ]\n",
      " [  54.      0.    -17.    -18.    -10.      3.      4.      2.    -17.08]\n",
      " [   1.      8.    -21.    -22.    -11.      4.      4.      1.    -25.07]\n",
      " [   1.      7.    -25.    -24.    -12.      4.      2.      2.    -36.06]\n",
      " [   1.      6.    -28.    -26.    -13.      3.      2.      2.    -42.05]\n",
      " [   1.      5.    -32.    -28.    -13.      4.      2.      1.    -48.04]\n",
      " [   1.      4.    -36.    -29.    -14.      4.      1.      2.    -58.03]\n",
      " [   1.      3.    -38.    -31.    -15.      2.      2.      2.    -63.02]\n",
      " [   1.      2.    -40.    -33.    -16.      2.      2.      2.    -68.01]\n",
      " [   1.      1.    -43.    -34.    -17.      3.      1.      2.    -81.  ]\n",
      " [   0.      0.    -44.    -35.    -18.      1.      1.      2.    -73.  ]\n",
      " [   0.      0.    -46.    -37.    -20.      2.      2.      2.    -79.  ]\n",
      " [   0.      0.    -47.    -39.    -23.      1.      2.      3.    -85.  ]\n",
      " [   0.      0.    -49.    -41.    -25.      2.      2.      2.    -87.  ]\n",
      " [   0.      0.    -50.    -43.    -29.      1.      2.      4.    -94.  ]\n",
      " [   0.      0.    -52.    -45.    -32.      2.      2.      3.   -101.  ]\n",
      " [   0.      0.    -53.    -47.    -36.      1.      2.      4.   -112.  ]\n",
      " [   0.      0.    -54.    -49.    -39.      1.      2.      3.   -106.  ]\n",
      " [   0.      0.    -56.    -53.    -42.      2.      4.      3.   -111.  ]\n",
      " [   0.      0.    -58.    -57.    -46.      2.      4.      4.      0.  ]]\n",
      "========= Step:  23 =========\n",
      "State:          [  0 -56 -53 -42   2   4   3]\n",
      "Action:         [0 0 0 0]\n",
      "Reward:         -111.0\n",
      "Next state:     [  0 -58 -57 -46   2   4   4]\n",
      "Episode reward: -180.2\n",
      "Average reward:  -149.07\n"
     ]
    }
   ],
   "source": [
    "%run evaluate_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
